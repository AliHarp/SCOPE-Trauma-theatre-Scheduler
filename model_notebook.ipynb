{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da456a4-7d3c-41e9-86c4-11cc0939a037",
   "metadata": {},
   "source": [
    "# Scheduling model for hips and ambulatory surgery\n",
    "\n",
    "Patients with hip fractures and ambulatory fractures arrive into the model. They have surgical deadlines applied. Patients are scheduled according to a 'slack' rule divided by a weighting, which is defined as:\n",
    "\n",
    "Time to deadline / weight.\n",
    "\n",
    "Hips are prioritised over ambulatory patients, but 'slack' ensures that ambulatory patients get seen.\n",
    "\n",
    "An optional 'duration' parameter will try to schedule longer patients in first.\n",
    "\n",
    "> Question: do we need an end of day 'packer' to fit short cases into remaining time? Done as utilisation was low in many cases.\n",
    "> \n",
    "> Question: breaches for served patients occur when their wait at service start exceeds deadline. This might need to change so that breaches are counted at the time of breaching. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432f697-7d3f-4587-b839-30ba62717fa3",
   "metadata": {},
   "source": [
    "# to do:\n",
    "1. 12 week rolling rota\n",
    "2. conversion to elective cancellations - >10 total; >5 hips\n",
    "3. Dynamic prioritisation >12 days for ambulatory\n",
    "4. Data and amb types\n",
    "5. ?Do all queueing patients PI need to be recalculated each day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb67031-ef91-4a30-b623-d48b76192dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field, replace, is_dataclass\n",
    "from typing import Dict, List, Deque, Any, Tuple, Callable, Optional, Iterable\n",
    "import math\n",
    "import copy\n",
    "from collections import deque, defaultdict\n",
    "from pathlib import Path\n",
    "from sim_tools.distributions import Exponential, Normal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import itertools\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25af5ba2-47cc-4fd6-a5aa-9deb541d8e82",
   "metadata": {},
   "source": [
    "## `Config` contains all the parameters used to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e3401b-4ed1-4c7e-b54e-e4f7109bdb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMB_KINDS = (\"shoulder\", \"wrist\", \"ankle\")\n",
    "ALL_KINDS = (\"hip\",) + AMB_KINDS  # (\"hip\", \"shoulder\", \"wrist\", \"ankle\")\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "\n",
    "    horizon_days: int = 182   # around 6 mths \n",
    "    base_seed: int = 1984\n",
    "    warmup_days: int = 21 # allow build-up of amb patients\n",
    "    \n",
    "    # ================= Capacity / arrivals =================\n",
    "    # Number of sessions per day. 0 = Monday\n",
    "    # Each instance gets its own new dictionary\n",
    "    trauma_sessions_by_dow: Dict[int, float] = field(\n",
    "        default_factory=lambda: {0: 4.0, 1: 2.0, 2: 4.0, 3: 2.0, 4: 4.0, 5: 2.0, 6: 2.0}\n",
    "    )\n",
    "    session_minutes: int = 240 #ie half day (minutes)\n",
    "    # interarrival times (minutes)\n",
    "    iat_mean_per_day: Dict[str, float] = field(\n",
    "        default_factory=lambda: {\n",
    "            \"hip\": 360.0,\n",
    "            \"shoulder\": 7200.0,\n",
    "            \"wrist\": 1440.0,\n",
    "            \"ankle\": 720.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # ================= Case durations =================\n",
    "    # mean, sd and min values (minutes)\n",
    "    duration_params: Dict[str, Dict[str, float]] = field(\n",
    "        default_factory=lambda: {\n",
    "            \"hip\":      {\"mean\": 120.0, \"sd\": 54.0, \"min\": 30.0},\n",
    "            \"shoulder\": {\"mean\": 55.0,  \"sd\": 22.5, \"min\": 20.0},\n",
    "            \"wrist\":    {\"mean\": 40.0,  \"sd\": 18.0, \"min\": 15.0},\n",
    "            \"ankle\":    {\"mean\": 40.0,  \"sd\": 27.0, \"min\": 20.0},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # ================= Breach deadlines =================\n",
    "    # 36 hours for hips, 14 days for ambulatory cases\n",
    "    breach_deadline_minutes: Dict[str, int] = field(\n",
    "        default_factory=lambda: {\n",
    "            \"hip\": 36 * 60,\n",
    "            \"shoulder\": 14 * 24 * 60,\n",
    "            \"wrist\": 14 * 24 * 60,\n",
    "            \"ankle\": 14 * 24 * 60,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # ================= Service policy =================\n",
    "    # Up for discussion - edd is just weighted slack (time to deadline); \n",
    "    # pi includes expected (not sampled) duration - weighs longer surgeries as more urgent\n",
    "    # hip can get a higher weight (denominator) so higher priority (lower edd/pi)\n",
    "    # start with no weights at all\n",
    "    service_policy: str = \"edd\"  # \"edd\" or \"pi\"\n",
    "    priority_weights: Dict[str, float] = field(\n",
    "        default_factory=lambda: {\"hip\": 2.0, \"shoulder\": 1.0, \"wrist\": 1.0, \"ankle\": 1.0}\n",
    "    )\n",
    "\n",
    "    # ================= utils =======================\n",
    "    TRACE: bool = False\n",
    "\n",
    "    def trace(self, *args, **kwargs) -> None:\n",
    "        if not getattr(self, \"TRACE\", False): return\n",
    "        msg = \" \".join(str(a) for a in args)\n",
    "        if kwargs:\n",
    "            details = \" \".join(f\"{k}={v!r}\" for k,v in kwargs.items())\n",
    "            print(f\"[TRACE] {msg} {details}\")\n",
    "        else:\n",
    "            print(f\"[TRACE] {msg}\")\n",
    "\n",
    "    \n",
    "    #################################################\n",
    "    # ================= VALIDATION  =================\n",
    "    #################################################\n",
    "\n",
    "\n",
    "    def validate(self, debug: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Validate configuration for the theatre scheduling simulation.\n",
    "    \n",
    "        Args:\n",
    "            debug (bool): If True, prints detailed debug information at each step.\n",
    "    \n",
    "        Raises:\n",
    "            ValueError: if validation fails.\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        warnings = []\n",
    "    \n",
    "        def dbg(msg):\n",
    "            if debug:\n",
    "                print(f\"[validate] {msg}\")\n",
    "    \n",
    "        def err(msg):\n",
    "            errors.append(str(msg))\n",
    "            if debug:\n",
    "                print(f\"[ERROR] {msg}\")\n",
    "    \n",
    "        def warn(msg):\n",
    "            warnings.append(str(msg))\n",
    "            if debug:\n",
    "                print(f\"[WARN] {msg}\")\n",
    "    \n",
    "        dbg(\"Starting config validation...\")\n",
    "    \n",
    "        # -------------------- basic scalars --------------------\n",
    "        dbg(\"Checking warmup_days and horizon_days...\")\n",
    "        warmup_days  = self.get_warmup_days()\n",
    "        horizon_days = self.get_horizon_days()\n",
    "        if warmup_days < 0:\n",
    "            err(\"warmup_days must be >= 0.\")\n",
    "        if horizon_days <= 0:\n",
    "            err(\"horizon_days must be > 0.\")\n",
    "        if horizon_days and warmup_days >= horizon_days:\n",
    "            err(f\"warmup_days ({warmup_days}) must be < horizon_days ({horizon_days}).\")\n",
    "        dbg(f\"warmup_days={warmup_days}, horizon_days={horizon_days}\")\n",
    "    \n",
    "        # base_seed\n",
    "        base_seed = self.get_base_seed()\n",
    "        try:\n",
    "            base_seed = int(base_seed)\n",
    "            dbg(f\"base_seed={base_seed}\")\n",
    "        except Exception:\n",
    "            err(f\"base_seed must be castable to int, got: {base_seed!r}\")\n",
    "    \n",
    "        # policy\n",
    "        policy = self.get_policy()\n",
    "        dbg(f\"service_policy={policy}\")\n",
    "        if policy not in {\"edd\", \"pi\"}:\n",
    "            err(f\"service_policy must be 'edd' or 'pi', got: {policy!r}\")\n",
    "    \n",
    "        # priority_weights\n",
    "        dbg(\"Checking priority_weights...\")\n",
    "        pweights = self.get_priority_weights()\n",
    "        if not isinstance(pweights, dict):\n",
    "            err(\"priority_weights must be a dict of {kind: positive number}.\")\n",
    "        else:\n",
    "            for k, v in pweights.items():\n",
    "                try:\n",
    "                    fv = float(v)\n",
    "                    dbg(f\"priority_weights[{k}]={fv}\")\n",
    "                    if fv <= 0:\n",
    "                        err(f\"priority_weights[{k!r}] must be > 0, got {v!r}\")\n",
    "                except Exception:\n",
    "                    err(f\"priority_weights[{k!r}] must be numeric, got {v!r}\")\n",
    "    \n",
    "        # -------------------- sessions / capacity --------------------\n",
    "        dbg(\"Checking session structure and minutes...\")\n",
    "        try:\n",
    "            sessions_by_dow = self.get_sessions_by_dow()\n",
    "            dbg(f\"sessions_by_dow={sessions_by_dow}\")\n",
    "            for dow, n in sessions_by_dow.items():\n",
    "                fn = float(n)\n",
    "                if fn < 0:\n",
    "                    err(f\"sessions_by_dow[{dow}] must be >= 0, got {n!r}\")\n",
    "        except Exception as e:\n",
    "            err(f\"get_sessions_by_dow() failed: {e}\")\n",
    "            sessions_by_dow = {}\n",
    "    \n",
    "        try:\n",
    "            session_minutes = self.get_session_minutes()\n",
    "            if callable(session_minutes):\n",
    "                val = float(session_minutes(0))\n",
    "            else:\n",
    "                val = float(session_minutes)\n",
    "            dbg(f\"session_minutes example={val}\")\n",
    "            if val <= 0:\n",
    "                err(f\"session_minutes must be > 0, got {val!r}\")\n",
    "        except Exception as e:\n",
    "            err(f\"get_session_minutes() failed: {e}\")\n",
    "            session_minutes = 0\n",
    "    \n",
    "        # quick weekly capacity sanity check\n",
    "        try:\n",
    "            sm = float(session_minutes(0)) if callable(session_minutes) else float(session_minutes)\n",
    "            total_weekly_capacity = sum(float(sessions_by_dow.get(d, 0.0)) * sm for d in range(7))\n",
    "            dbg(f\"total_weekly_capacity={total_weekly_capacity}\")\n",
    "            if total_weekly_capacity == 0:\n",
    "                warn(\"Total weekly capacity is 0 minutes (all sessions_by_dow are zero).\")\n",
    "        except Exception as e:\n",
    "            warn(f\"Could not compute total weekly capacity: {e}\")\n",
    "    \n",
    "        # -------------------- clinical models / kinds alignment --------------------\n",
    "        dbg(\"Validating duration model...\")\n",
    "        try:\n",
    "            duration_model = self.get_duration_model()\n",
    "            dbg(f\"Duration kinds={list(duration_model.keys())}\")\n",
    "            for kind, params in duration_model.items():\n",
    "                dbg(f\"Checking duration for kind={kind}: {params}\")\n",
    "                m = float(params.get(\"mean\", 0))\n",
    "                s = float(params.get(\"sd\", 0))\n",
    "                if m <= 0:\n",
    "                    err(f\"Duration mean for {kind!r} must be > 0, got {m!r}\")\n",
    "                if s < 0:\n",
    "                    err(f\"Duration sd for {kind!r} must be >= 0, got {s!r}\")\n",
    "        except Exception as e:\n",
    "            err(f\"get_duration_model() failed: {e}\")\n",
    "            duration_model = {}\n",
    "    \n",
    "        dbg(\"Validating breach deadlines...\")\n",
    "        try:\n",
    "            deadlines_min = self.get_breach_deadlines_minutes()\n",
    "            dbg(f\"Deadlines={deadlines_min}\")\n",
    "            for kind, mins in deadlines_min.items():\n",
    "                dm = int(mins)\n",
    "                if dm <= 0:\n",
    "                    err(f\"Deadline for {kind!r} must be > 0 minutes, got {mins!r}\")\n",
    "        except Exception as e:\n",
    "            err(f\"get_breach_deadlines_minutes() failed: {e}\")\n",
    "            deadlines_min = {}\n",
    "    \n",
    "        # -------------------- arrival model --------------------\n",
    "        dbg(\"Validating arrivals (iat_mean_per_day)...\")\n",
    "        try:\n",
    "            arrivals = self.get_iat_mean_per_day()  # {kind: float minutes}\n",
    "            dbg(f\"Arrival kinds={list(arrivals.keys())}\")\n",
    "            for kind, iat in arrivals.items():\n",
    "                try:\n",
    "                    f = float(iat)\n",
    "                    if f <= 0:\n",
    "                        err(f\"iat_mean_per_day for '{kind}' must be > 0 minutes, got {iat!r}\")\n",
    "                except Exception:\n",
    "                    err(f\"iat_mean_per_day for '{kind}' must be numeric, got {iat!r}\")\n",
    "        except Exception as e:\n",
    "            err(f\"get_iat_mean_per_day() failed: {e}\")\n",
    "            arrivals = {}\n",
    "    \n",
    "        # -------------------- kinds alignment --------------------\n",
    "        dbg(\"Checking kind alignment between models...\")\n",
    "        kinds_from_dur = set(duration_model.keys())\n",
    "        kinds_from_dead = set(deadlines_min.keys())\n",
    "        kinds_from_arr = set(arrivals.keys()) if isinstance(arrivals, dict) else set()\n",
    "    \n",
    "        missing_in_dead = kinds_from_dur - kinds_from_dead\n",
    "        missing_in_dur  = kinds_from_dead - kinds_from_dur\n",
    "        if missing_in_dead:\n",
    "            err(f\"Deadlines missing kinds present in duration model: {sorted(missing_in_dead)}\")\n",
    "        if missing_in_dur:\n",
    "            err(f\"Duration model missing kinds present in deadlines: {sorted(missing_in_dur)}\")\n",
    "    \n",
    "        if kinds_from_arr:\n",
    "            for k in kinds_from_arr - kinds_from_dur:\n",
    "                warn(f\"Arrivals define kind {k!r} not present in duration model.\")\n",
    "            for k in kinds_from_arr - kinds_from_dead:\n",
    "                warn(f\"Arrivals define kind {k!r} not present in deadlines mapping.\")\n",
    "    \n",
    "        # -------------------- sampler smoke test --------------------\n",
    "\n",
    "        dbg(\"Mean minutes sanity check...\")\n",
    "        try:\n",
    "            means = self.get_mean_case_minutes()  # deterministic, no RNG\n",
    "            for k, m in means.items():\n",
    "                if not (m > 0 and m < 1e6):\n",
    "                    warn(f\"Mean case minutes for '{k}' looks suspicious: {m}\")\n",
    "            dbg(f\"mean_case_minutes={means}\")\n",
    "        except Exception as e:\n",
    "            warn(f\"Mean minutes check failed: {e}\")\n",
    "    \n",
    "        # -------------------- wrap up --------------------\n",
    "        if errors:\n",
    "            print(\"\\n❌ CONFIG VALIDATION FAILED ❌\")\n",
    "            for e in errors:\n",
    "                print(\"  -\", e)\n",
    "            raise ValueError(f\"{len(errors)} error(s) during config validation.\")\n",
    "        else:\n",
    "            print(\"✅ Config validation passed without critical errors.\")\n",
    "    \n",
    "        if warnings:\n",
    "            print(\"\\n⚠️ WARNINGS:\")\n",
    "            for w in warnings:\n",
    "                print(\"  -\", w)\n",
    "    \n",
    "        if debug:\n",
    "            print(\"[validate] Completed successfully.\")\n",
    "\n",
    "    ###########################################\n",
    "    # ----------------------------- Getters ------\n",
    "    ############################################\n",
    "\n",
    "    #constants\n",
    "    def get_horizon_days(self) -> int:\n",
    "        return int(self.horizon_days)\n",
    "\n",
    "    def get_warmup_days(self) -> int:\n",
    "        # pick one canonical field name and stick to it\n",
    "        return int(getattr(self, \"warmup_days\", 0))\n",
    "    \n",
    "    def get_warmup_cut_min(self) -> int:\n",
    "        return self.get_warmup_days() * 1440\n",
    "    \n",
    "    def get_horizon_end_min(self) -> int:\n",
    "        return self.get_horizon_days() * 1440\n",
    "\n",
    "    # surgical kinds\n",
    "    def get_kinds(self) -> list[str]:\n",
    "        \"\"\"Canonical kind list used across models.\"\"\"\n",
    "        return sorted(self.get_duration_model().keys())\n",
    "    \n",
    "    #policy and weights\n",
    "    def get_policy(self) -> str:\n",
    "        p = str(getattr(self, \"service_policy\", \"edd\")).lower()\n",
    "        return p if p in (\"edd\",\"pi\") else \"edd\"\n",
    "    \n",
    "    def get_priority_weights(self) -> dict[str, float]:\n",
    "        # normalized, positive (>0), missing -> 1.0\n",
    "        kinds = self.get_kinds()\n",
    "        raw = dict(getattr(self, \"priority_weights\", {}) or {})\n",
    "        out = {}\n",
    "        for k in kinds:\n",
    "            v = float(raw.get(k, 1.0))\n",
    "            out[k] = v if v > 0 else 1.0\n",
    "        return out\n",
    "    \n",
    "    def get_weight_for(self, kind: str) -> float:\n",
    "        return self.get_priority_weights().get(str(kind), 1.0)\n",
    "\n",
    "    #Durations\n",
    "\n",
    "    def get_duration_model(self) -> Dict[str, dict]:\n",
    "        \"\"\"\n",
    "        Return duration specs per kind directly from cfg.duration_params.\n",
    "        Each entry is {kind: {\"mean\": float, \"sd\": float, \"min\": float}}.\n",
    "        \"\"\"\n",
    "        out: Dict[str, dict] = {}\n",
    "        for kind, p in self.duration_params.items():\n",
    "            m  = float(p[\"mean\"])\n",
    "            sd = float(p[\"sd\"])\n",
    "            mn = float(p.get(\"min\", 0.0))\n",
    "            out[str(kind)] = {\"mean\": m, \"sd\": sd, \"min\": mn}\n",
    "        return out\n",
    "    \n",
    "    def get_mean_case_minutes(self) -> dict[str, float]:\n",
    "        dm = self.get_duration_model()\n",
    "        return {k: float(v[\"mean\"]) for k, v in dm.items()}\n",
    "    \n",
    "\n",
    "    #Deadlines\n",
    "    def get_breach_deadlines_minutes(self) -> dict[str, int]:\n",
    "        return {str(k): int(v) for k, v in self.breach_deadline_minutes.items()}\n",
    "    \n",
    "    def get_deadline_for(self, kind: str) -> int:\n",
    "        return int(self.get_breach_deadlines_minutes()[str(kind)])\n",
    "\n",
    "    #Arrivals\n",
    "    def get_iat_mean_per_day(self) -> dict[str, float]:\n",
    "        return {str(k): float(v) for k, v in self.iat_mean_per_day.items()}\n",
    "    \n",
    "    def get_iat_for(self, kind: str) -> float:\n",
    "        return float(self.get_iat_mean_per_day()[str(kind)])\n",
    "    \n",
    "    def get_arrival_rate_per_min(self) -> dict[str, float]:\n",
    "        # convenience: lambda per-minute rate = 1 / mean interarrival minutes\n",
    "        iat = self.get_iat_mean_per_day()\n",
    "        return {k: (1.0 / v if v > 0 else 0.0) for k, v in iat.items()}\n",
    "    \n",
    "    # Capacity, calendar\n",
    "\n",
    "    def get_sessions_by_dow(self) -> dict[int, float]:\n",
    "        return {int(k): float(v) for k, v in self.trauma_sessions_by_dow.items()}\n",
    "    \n",
    "    def get_session_minutes(self):\n",
    "        # if you later support a callable, keep this as-is and branch at call site\n",
    "        return int(self.session_minutes)\n",
    "    \n",
    "    def capacity_minutes_for_day(self, day: int) -> float:\n",
    "        dow = day % 7\n",
    "        sessions_today = float(self.get_sessions_by_dow().get(dow, 0.0))\n",
    "        sm = self.get_session_minutes()\n",
    "        m_per_session = float(sm(day)) if callable(sm) else float(sm)\n",
    "        return sessions_today * m_per_session\n",
    "    \n",
    "    def get_total_weekly_capacity_minutes(self) -> float:\n",
    "        sm = self.get_session_minutes()\n",
    "        m_per_session = float(sm(0)) if callable(sm) else float(sm)\n",
    "        return sum(float(self.get_sessions_by_dow().get(d, 0.0)) * m_per_session for d in range(7))\n",
    "    \n",
    "    # seeds\n",
    "    def get_base_seed(self) -> int:\n",
    "        return int(getattr(self, \"base_seed\", 0) or 0)\n",
    "\n",
    "    def seed_for(self, run_id: int, *, stream: str = \"\") -> int:\n",
    "        # deterministic, per-stream seeds (e.g., \"arrivals\", \"durations\")\n",
    "        # avoids cross-correlation between processes\n",
    "        tag = (hash(stream) & 0x7fffffff)\n",
    "        return self.get_base_seed() + int(run_id) + tag\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e5056-4300-4e96-a5c5-56cc8b5796d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ef4af-479b-4e3b-9bf1-d2c174547acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "cfg.validate(debug=True)\n",
    "\n",
    "print(cfg.get_kinds())\n",
    "print(cfg.get_sessions_by_dow())\n",
    "print(cfg.get_session_minutes())\n",
    "print(cfg.get_iat_mean_per_day())\n",
    "print(cfg.get_duration_model())\n",
    "print(cfg.get_breach_deadlines_minutes())\n",
    "print(cfg.get_mean_case_minutes())\n",
    "print(cfg.capacity_minutes_for_day(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19fccf-0388-4427-87ce-77c9e9ffdb2d",
   "metadata": {},
   "source": [
    "## Daily planner\n",
    "\n",
    "Two simple scheduling rules are used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ff3be2-00c5-4c0c-aa7b-98cb0e011c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DailyServer:\n",
    "    \"\"\"\n",
    "    Schedules patients for a single day using a priority policy (\"edd\" or \"pi\"),\n",
    "    with dynamic (current-time) slack, excluding not-yet-arrived patients.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, trace: Callable[..., None] = lambda *a, **k: None):\n",
    "        self.trace = trace\n",
    "\n",
    "    def serve_day(\n",
    "        self,\n",
    "        *,\n",
    "        queues: Dict[str, Deque[Tuple[int, int]]],           # per kind: (arrival_min, patient_id)\n",
    "        minutes_budget: float,                                # operating minutes available today\n",
    "        day_start_min: int,\n",
    "        policy: str,                                          # \"edd\" or \"pi\"\n",
    "        deadlines_min: Dict[str, int],                        # per-kind breach deadlines (minutes)\n",
    "        duration_sampler: Dict[str, Callable[[], float]],     # kind -> sampler() -> duration (minutes)\n",
    "        mean_case_minutes: Dict[str, float],                  # kind -> E[duration] (for PI index)\n",
    "        priority_weights: Dict[str, float] | None = None,     # higher weight => higher priority\n",
    "        prev_day_start_min: int | None = None,                # to compute daily breach incidence\n",
    "    ) -> Tuple[\n",
    "        float,                              # minutes_left_today\n",
    "        Dict[str, int],                     # served_today\n",
    "        Dict[str, int],                     # breached_today (served while in breach)\n",
    "        Dict[str, List[int]],               # waits_today (true wait to service start)\n",
    "        Dict[str, List[int]],               # excess_today (minutes over deadline)\n",
    "        List[Dict[str, float | int | str | bool]],  # patient_records\n",
    "        Dict[str, int],                     # in_breach_start\n",
    "        Dict[str, int],                     # new_breaches_today\n",
    "    ]:\n",
    "        tr = self.trace\n",
    "        pol = (policy or \"edd\").lower()\n",
    "        if pol not in (\"edd\", \"pi\"):\n",
    "            raise ValueError(f\"Unknown policy '{policy}'. Use 'edd' or 'pi'.\")\n",
    "\n",
    "        tr(\"start DailyServer.serve_day\", minutes_budget=minutes_budget, policy=pol)\n",
    "\n",
    "        # --- Daily breach snapshots (computed even if no capacity today) ---\n",
    "        in_breach_start: Dict[str, int]    = {k: 0 for k in queues}\n",
    "        new_breaches_today: Dict[str, int] = {k: 0 for k in queues}\n",
    "\n",
    "        for kind, q in queues.items():\n",
    "            if not q:\n",
    "                continue\n",
    "            dline = int(deadlines_min.get(kind, 10**9))\n",
    "            for arr, pid in q:\n",
    "                if arr >= day_start_min:\n",
    "                    continue  # not yet waiting at day start\n",
    "                wait_so_far = day_start_min - arr\n",
    "                if wait_so_far > dline:          # strict '>' matches serve-time logic or should equality count?\n",
    "                    in_breach_start[kind] += 1   # counts breaches as they happen\n",
    "                    tr(\"breach at start\", kind=kind, pid=pid, wait_so_far=wait_so_far, deadline=dline)\n",
    "                if prev_day_start_min is not None:\n",
    "                    breach_time = arr + dline\n",
    "                    if prev_day_start_min < breach_time <= day_start_min:\n",
    "                        new_breaches_today[kind] += 1 # breaches today regardless of served\n",
    "                        tr(\"new_breach_today\", kind=kind, pid=pid, breach_time=breach_time)\n",
    "\n",
    "        # If no theatre time, still return the daily breach snapshot\n",
    "        if minutes_budget <= 0:\n",
    "            empty_counts = {k: 0 for k in queues}\n",
    "            empty_lists  = {k: [] for k in queues}\n",
    "            return (minutes_budget, empty_counts, empty_counts,\n",
    "                    empty_lists, empty_lists, [],\n",
    "                    in_breach_start, new_breaches_today)\n",
    "\n",
    "        served_today: Dict[str, int]        = {k: 0 for k in queues}\n",
    "        breached_today: Dict[str, int]      = {k: 0 for k in queues}\n",
    "        waits_today: Dict[str, List[int]]   = {k: [] for k in queues}\n",
    "        excess_today: Dict[str, List[int]]  = {k: [] for k in queues}\n",
    "        patient_records: List[Dict[str, float | int | str | bool]] = []\n",
    "\n",
    "        weights = dict(priority_weights or {})\n",
    "        mean_dur = {k: float(mean_case_minutes.get(k, 1.0)) for k in queues}\n",
    "        initial_budget = float(minutes_budget)\n",
    "\n",
    "        def weight_for(kind: str) -> float:\n",
    "            w = float(weights.get(kind, 1.0))\n",
    "            return w if w > 0 else 1.0\n",
    "\n",
    "        # >>> Track absolute time explicitly\n",
    "        current_time = int(day_start_min)\n",
    "        day_end_abs = int(day_start_min + initial_budget)\n",
    "\n",
    "        while minutes_budget > 0:\n",
    "            tr(\"Building candidates list\", current_time=current_time, minutes_left=round(minutes_budget,1))\n",
    "            candidates: List[Tuple[float, int, str, Tuple[int, int]]] = []  # (idx, arrival, kind, (arr,pid))\n",
    "\n",
    "            for kind, q in queues.items():\n",
    "                if not q:\n",
    "                    continue\n",
    "                dline = deadlines_min.get(kind, 10**9)\n",
    "                w = weight_for(kind)\n",
    "                e_dur = mean_dur.get(kind, 1.0)\n",
    "                for arr, pid in q:\n",
    "                    if arr > current_time:\n",
    "                        continue  # not yet arrived\n",
    "                    wait_now = current_time - arr\n",
    "                    slack = dline - wait_now\n",
    "                    if pol == \"pi\":\n",
    "                        denom = max(1.0, e_dur * w)\n",
    "                        idx = slack / denom\n",
    "                    else:\n",
    "                        idx = slack / max(w, 1e-9)\n",
    "                    candidates.append((idx, arr, kind, (arr, pid)))\n",
    "\n",
    "            if candidates:\n",
    "                idx_vals = [c[0] for c in candidates]\n",
    "                min_idx = min(idx_vals)\n",
    "                ties = sum(1 for v in idx_vals if abs(v - min_idx) < 1e-9)\n",
    "                tr(\"Candidates summary\", n=len(candidates), min_idx=round(min_idx, 3), ties_at_min=ties)\n",
    "\n",
    "            if not candidates:\n",
    "                # FAST-FORWARD TO NEXT ARRIVAL (if any) \n",
    "                next_arrival = None\n",
    "                for q in queues.values():\n",
    "                    if q:\n",
    "                        arr0 = q[0][0]  # earliest in that queue\n",
    "                        if arr0 > current_time:\n",
    "                            next_arrival = arr0 if next_arrival is None else min(next_arrival, arr0)\n",
    "\n",
    "                if (next_arrival is not None) and (next_arrival <= day_end_abs):\n",
    "                    tr(\"Fast-forwarding to next arrival\",\n",
    "                       from_time=current_time, to_time=next_arrival,\n",
    "                       within_operating_window=True)\n",
    "                    current_time = int(next_arrival)\n",
    "                    # note: minutes_budget unchanged (idle time does not consume theatre minutes)\n",
    "                    continue\n",
    "\n",
    "                tr(\"No candidates left, break day loop\",\n",
    "                   current_time=current_time, day_end_abs=day_end_abs)\n",
    "                break\n",
    "\n",
    "            # Pick the best candidate - next line, if tie, who arrived first\n",
    "            candidates.sort(key=lambda t: (t[0], t[1]))   # idx asc, then earlier arrival on ties\n",
    "            idx_sel, _arr_key, kind, (arr, pid) = candidates[0]\n",
    "            tr(\"Next patient\", kind=kind, pid=pid, idx=idx_sel)\n",
    "\n",
    "            # surgery duration\n",
    "            dur = float(duration_sampler[kind]())\n",
    "            tr(\"Sampled duration\", kind=kind, pid=pid, dur=round(dur,1), minutes_budget_left=round(minutes_budget,1))\n",
    "\n",
    "            # End-of-day packer: try to fit a smaller case if the top one doesn't fit\n",
    "            # Utilisation is low if FIFO by PI is used.\n",
    "            if minutes_budget < dur:\n",
    "                tr(\"Top candidate does not fit\",\n",
    "                   top_kind=kind, top_pid=pid, top_dur=round(dur,1),\n",
    "                   minutes_left=round(minutes_budget,1))\n",
    "                packed = False\n",
    "                for try_idx, (idx2, _arr2, kind2, (arr2, pid2)) in enumerate(candidates[1:11], start=1):\n",
    "                    dur2 = float(duration_sampler[kind2]())\n",
    "                    tr(\"Packer try\", attempt=try_idx, cand_kind=kind2, cand_pid=pid2,\n",
    "                       cand_dur=round(dur2,1), minutes_left=round(minutes_budget,1))\n",
    "                    if minutes_budget >= dur2:\n",
    "                        kind, arr, pid, dur = kind2, arr2, pid2, dur2\n",
    "                        packed = True\n",
    "                        tr(\"Packer chose\", kind=kind, pid=pid, dur=round(dur,1))\n",
    "                        break\n",
    "                if not packed:\n",
    "                    tr(\"Packer failed: stopping for the day\",\n",
    "                       minutes_left=round(minutes_budget,1))\n",
    "                    break\n",
    "\n",
    "            dline = deadlines_min.get(kind, 10**9)\n",
    "\n",
    "            # True service times & true wait\n",
    "            service_start = int(current_time)\n",
    "            service_end   = int(round(service_start + dur))\n",
    "            wait_true     = service_start - arr\n",
    "            slack_now     = dline - wait_true\n",
    "            breached_now  = (wait_true > dline)\n",
    "            breach_excess = max(0, wait_true - dline)\n",
    "\n",
    "            # Transparent logging\n",
    "            wait_start  = day_start_min - arr\n",
    "            slack_start = dline - wait_start\n",
    "            tr(\"Pick\", kind=kind, pid=pid, idx_raw=round(idx_sel,3),\n",
    "               slack_start=int(slack_start), mean_dur=mean_dur.get(kind,1.0),\n",
    "               weight=weight_for(kind), dur_sampled=round(dur,1))\n",
    "            tr(\"Serving patient\", kind=kind, pid=pid, wait=wait_true,\n",
    "               dur=round(dur,1), slack_now=round(slack_now,1), deadline=dline)\n",
    "\n",
    "            # Serve: consume theatre minutes and advance absolute time\n",
    "            queues[kind].remove((arr, pid))\n",
    "            minutes_budget -= dur\n",
    "            current_time = service_end\n",
    "\n",
    "            served_today[kind] += 1\n",
    "            waits_today[kind].append(int(wait_true))\n",
    "            if breached_now:\n",
    "                breached_today[kind] += 1\n",
    "                excess_today[kind].append(int(breach_excess))\n",
    "\n",
    "            # Day progress summary (util uses only used minutes, not idle)\n",
    "            used = initial_budget - minutes_budget\n",
    "            util = 0.0 if initial_budget <= 0 else used / initial_budget\n",
    "            tr(\"Day summary\",\n",
    "               served=sum(served_today.values()),\n",
    "               served_breached=sum(breached_today.values()),\n",
    "               minutes_left=round(minutes_budget,1),\n",
    "               used=round(used,1),\n",
    "               util=f\"{util:.1%}\")\n",
    "\n",
    "            # Patient record\n",
    "            patient_records.append({\n",
    "                \"id\": pid,\n",
    "                \"kind\": kind,\n",
    "                \"arrival\": int(arr),\n",
    "                \"service_start\": service_start,\n",
    "                \"service_end\": service_end,\n",
    "                \"wait\": int(wait_true),\n",
    "                \"duration\": float(dur),\n",
    "                \"deadline\": int(dline),\n",
    "                \"breached\": bool(breached_now),                   # true when wait >= deadline\n",
    "                \"excess\": int(breach_excess),                     # 0 if below deadline\n",
    "                \"policy\": pol,\n",
    "                \"breach_time\": int(arr + dline),\n",
    "            })\n",
    "\n",
    "        return (minutes_budget, served_today, breached_today,\n",
    "                waits_today, excess_today, patient_records,\n",
    "                in_breach_start, new_breaches_today)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9745510b-d6ca-47a0-ba4f-d6f381ea385f",
   "metadata": {},
   "source": [
    "# Sampling arrivals and surgical duration\n",
    "\n",
    "Using sim-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de35a1e-6f33-4bbf-9c21-69b9057e476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DurationSampler:\n",
    "    \"\"\"\n",
    "    Holds per-kind duration samplers using Config.duration_params.\n",
    "    Uses Normal(mean, sigma, minimum, random_seed) from sim-tools\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, rng_seed: int | None, trace=lambda *a, **k: None):\n",
    "        self.cfg = cfg\n",
    "        self.trace = trace\n",
    "        self.rng_seed = 0 if rng_seed is None else int(rng_seed)\n",
    "        self._samplers: Dict[str, Callable[[], float]] = self._build()\n",
    "\n",
    "    def sample(self, kind: str) -> float:\n",
    "        try:\n",
    "            return float(self._samplers[kind]())\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"DurationSampler: unknown kind '{kind}'. \"\n",
    "                           f\"Known kinds: {sorted(self._samplers.keys())}\") from None\n",
    "\n",
    "    def mean_minutes(self) -> Dict[str, float]:\n",
    "        return self.cfg.get_mean_case_minutes()\n",
    "\n",
    "    def as_dict(self) -> Dict[str, Callable[[], float]]:\n",
    "        \"\"\"Optional adapter if your server wants {kind: () -> float}.\"\"\"\n",
    "        return self._samplers\n",
    "\n",
    "    def _build(self) -> Dict[str, Callable[[], float]]:\n",
    "        dur_model = self.cfg.get_duration_model()   # {kind: {mean, sd, min}}\n",
    "        kinds = sorted(dur_model.keys())            # stable order across runs\n",
    "        ss = np.random.SeedSequence(self.rng_seed)\n",
    "        seeds = ss.spawn(len(kinds))\n",
    "\n",
    "        samplers: Dict[str, Callable[[], float]] = {}\n",
    "        for kind, seed in zip(kinds, seeds):\n",
    "            p = dur_model[kind]\n",
    "            m, sd, mn = float(p[\"mean\"]), float(p[\"sd\"]), float(p.get(\"min\", 0.0))\n",
    "            # sim-tools with independent per-kind stream\n",
    "            dist = Normal(mean=m, sigma=sd, minimum=mn, random_seed=seed)\n",
    "            # late-binding safe via default arg\n",
    "            samplers[kind] = (lambda d=dist: float(d.sample()))\n",
    "            self.trace(\"Duration sampler built\",\n",
    "                       kind=kind, mean=m, sd=sd, minimum=mn, seed_entropy=seed.entropy)\n",
    "\n",
    "        self.trace(\"DurationSampler ready\",\n",
    "                   base_seed=self.rng_seed, kinds=kinds, n=len(kinds))\n",
    "        return samplers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5600fefc-d580-4a8a-a130-d655360b1ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArrivalProcess:\n",
    "    \"\"\"\n",
    "    Pre-generates sorted absolute arrival times per kind over a horizon,\n",
    "    using Exponential(mean_iat_minutes).\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, horizon_days: int, rng_seed: Optional[int], trace=lambda *a, **k: None):\n",
    "        self.cfg = cfg\n",
    "        self.trace = trace\n",
    "        self.horizon_days = int(horizon_days)\n",
    "        self.rng_seed = 0 if rng_seed is None else int(rng_seed)\n",
    "\n",
    "        self.times_by_kind: Dict[str, List[int]] = self._build()\n",
    "        self.next_idx: Dict[str, int] = {k: 0 for k in self.times_by_kind}\n",
    "\n",
    "        self.trace(\"ArrivalProcess ready\",\n",
    "                   base_seed=self.rng_seed,\n",
    "                   horizon_min=self.horizon_days * 1440,\n",
    "                   kinds=sorted(self.times_by_kind.keys()))\n",
    "\n",
    "    # -------- public API --------\n",
    "\n",
    "    def pop_arrivals_up_to(self, t_min: int) -> Dict[str, List[int]]:\n",
    "        \"\"\"Consume and return all arrivals with time <= t_min.\"\"\"\n",
    "        out = {k: [] for k in self.times_by_kind}\n",
    "        for k, times in self.times_by_kind.items():\n",
    "            i = self.next_idx[k]\n",
    "            n = len(times)\n",
    "            while i < n and times[i] <= t_min:\n",
    "                out[k].append(times[i])\n",
    "                i += 1\n",
    "            self.next_idx[k] = i\n",
    "        return out\n",
    "\n",
    "    def remaining(self, kind: str) -> int:\n",
    "        \"\"\"How many arrivals remain for a kind (not yet popped).\"\"\"\n",
    "        return len(self.times_by_kind[kind]) - self.next_idx[kind]\n",
    "\n",
    "    def peek_next(self, kind: str) -> Optional[int]:\n",
    "        \"\"\"Next unconsumed arrival time for a kind, else None.\"\"\"\n",
    "        i = self.next_idx[kind]\n",
    "        times = self.times_by_kind[kind]\n",
    "        return times[i] if i < len(times) else None\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset consumption indices (does not rebuild or reseed).\"\"\"\n",
    "        self.next_idx = {k: 0 for k in self.times_by_kind}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Total number of pre-generated arrivals (all kinds).\"\"\"\n",
    "        return sum(len(v) for v in self.times_by_kind.values())\n",
    "\n",
    "    # -------- internal --------\n",
    "\n",
    "    def _build(self) -> Dict[str, List[int]]:\n",
    "        horizon_min = self.horizon_days * 1440\n",
    "        mean_iats = self.cfg.get_iat_mean_per_day()  # {kind: float minutes}\n",
    "        kinds = sorted(mean_iats.keys())              # stable ordering\n",
    "\n",
    "        ss = np.random.SeedSequence(self.rng_seed)\n",
    "        seeds = ss.spawn(len(kinds))\n",
    "\n",
    "        times: Dict[str, List[int]] = {k: [] for k in kinds}\n",
    "        for kind, seed in zip(kinds, seeds):\n",
    "            mean_iat = float(mean_iats[kind])\n",
    "            if mean_iat <= 0:\n",
    "                self.trace(\"ArrivalProcess skip kind (non-positive iat)\", kind=kind, mean_iat=mean_iat)\n",
    "                continue\n",
    "\n",
    "            dist = Exponential(mean=mean_iat, random_seed=seed)\n",
    "            t = 0.0\n",
    "            cnt = 0\n",
    "            # cumulative sum of exponentials => naturally sorted; no need to sort afterwards\n",
    "            while t < horizon_min:\n",
    "                t += float(dist.sample())\n",
    "                if t >= horizon_min:\n",
    "                    break\n",
    "                times[kind].append(int(t))\n",
    "                cnt += 1\n",
    "\n",
    "            self.trace(\"Arrival stream built\",\n",
    "                       kind=kind, mean_iat=mean_iat, horizon_min=horizon_min,\n",
    "                       n=cnt, seed_entropy=seed.entropy)\n",
    "\n",
    "        return times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbfd841-7257-4fca-9eae-6080567eb257",
   "metadata": {},
   "source": [
    "## Run model function\n",
    "\n",
    "Run using either EDD or PI \n",
    "The priority index (PI) is used in scheduling theory:\n",
    "\n",
    "PI = slack / (mean duration * weight)\n",
    "\n",
    "slack is the time to deadline\n",
    "mean duration is the **expected** duration, not the sampled duration\n",
    "\n",
    "A smaller PI implies a more urgent job, pushed down by low slack (at or near deadline), long expected processing time, or a high weight.\n",
    "\n",
    "Shorter jobs raise the index (reduce the priority) and the weight multiplies that effect. \n",
    "\n",
    "For surgery, it may be that favouring shorter surgery will increase throughput, but hips take longer in general, and are operationally more urgent (tariffs).\n",
    "\n",
    "Prioritising longer surgery an introduce:  \n",
    "    * 1. A long idle tail at end of day\n",
    "    * FIX: try next candidate to find one that fits (end of day 'packer')  - done, check next ten in queue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9295f2b8-6d04-44fd-8519-371395955838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    \"\"\"\n",
    "    A single replication given a Config - no global state.\n",
    "    External dependencies (must be provided by caller/module):\n",
    "      - ArrivalProcess(cfg, horizon_days, rng_seed, trace)\n",
    "      - DurationSampler(cfg, rng_seed, trace) with .as_dict() -> {kind: () -> float} and .mean_minutes() -> {kind: float}\n",
    "      - DailyServer(trace) with .serve_day(...)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.trace = cfg.trace  # injected tracer: callable(msg: str, **kwargs)\n",
    "        self._validated = False #so can run validation() on first rep only\n",
    "\n",
    "    def _minutes_capacity_for_day(self, sessions_by_dow: Dict[int, float], session_minutes, day: int) -> float:\n",
    "        \"\"\"Compute theatre minutes available for a given day index.\"\"\"\n",
    "        dow = day % 7\n",
    "        sessions_today = float(sessions_by_dow.get(dow, 0.0))\n",
    "        m_per_session = float(session_minutes(day)) if callable(session_minutes) else float(session_minutes)\n",
    "        return sessions_today * m_per_session\n",
    "\n",
    "    def single_run(self, run_id: int = 1) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        One replication with policy = cfg.service_policy (\"edd\" or \"pi\").\n",
    "\n",
    "        Outputs:\n",
    "          - \"patients\": patient-level records for BOTH served and unserved patients.\n",
    "          - \"kpis\": run-level aggregates (post-warmup where noted), with detailed comments below.\n",
    "        \"\"\"\n",
    "        cfg = self.cfg\n",
    "        tr = self.trace\n",
    "\n",
    "        # --- Setup & config normalization ---\n",
    "        tr(\"Begin single_run\", run_id=run_id)\n",
    "        if not self._validated:\n",
    "            self.cfg.validate()\n",
    "            self._validated = True #after first run only\n",
    "\n",
    "        warmup_cut_min  = cfg.get_warmup_cut_min()\n",
    "        horizon_days    = cfg.get_horizon_days()\n",
    "        end_time_min    = cfg.get_horizon_end_min()\n",
    "        tr(\"Initial constants\",\n",
    "           warmup_days=cfg.get_warmup_days(),\n",
    "           warmup_cut_min=warmup_cut_min,\n",
    "           horizon_days=horizon_days,\n",
    "           horizon_end_min=end_time_min)\n",
    "\n",
    "        base_seed        = cfg.get_base_seed() + int(run_id)\n",
    "        sessions_by_dow  = cfg.get_sessions_by_dow()\n",
    "        session_minutes  = cfg.get_session_minutes()\n",
    "        deadlines_min    = cfg.get_breach_deadlines_minutes()\n",
    "        policy           = cfg.get_policy()\n",
    "        priority_weights = cfg.get_priority_weights()\n",
    "        tr(\"Base seed\", value=base_seed)\n",
    "        tr(\"Policy & weights\", policy=policy, weights=priority_weights)\n",
    "\n",
    "        # --- Stochastic inputs ---\n",
    "        arrivals = ArrivalProcess(cfg, horizon_days, rng_seed=base_seed + 10_000, trace=tr)\n",
    "        dur      = DurationSampler(cfg, rng_seed=base_seed + 20_000, trace=tr)\n",
    "        duration_sampler  = dur.as_dict()     # {kind: () -> float}\n",
    "        mean_case_minutes = dur.mean_minutes()# {kind: float}\n",
    "\n",
    "        # --- State ---\n",
    "        patient_id_counter: itertools.count = itertools.count(1)\n",
    "        queues: Dict[str, Deque[Tuple[int, int]]] = {k: deque() for k in arrivals.times_by_kind.keys()}\n",
    "\n",
    "        # --- Aggregates (post-warmup, by kind unless stated otherwise) ---\n",
    "        served_counts         = {k: 0 for k in queues}  # served patient counts by kind\n",
    "        breached_counts       = {k: 0 for k in queues}  # served-in-breach counts by kind\n",
    "        waits_all             = {k: [] for k in queues} # waits (minutes) for served patients by kind\n",
    "        excess_all            = {k: [] for k in queues} # excess over deadline (minutes) for served, by kind\n",
    "\n",
    "        in_breach_start_tot   = {k: 0 for k in queues}  # daily snapshot: already-breached at start-of-day, summed over post-warmup days\n",
    "        new_breaches_tot      = {k: 0 for k in queues}  # daily snapshot: newly crossed breach threshold during the day, summed post-warmup\n",
    "        in_breach_end_tot     = {k: 0 for k in queues}  # daily snapshot: still-waiting & in-breach at end-of-day, summed post-warmup\n",
    "\n",
    "        # Utilisation (post-warmup only)\n",
    "        util_minutes_used_total      = 0.0  # sum of minutes actually used across post-warmup days\n",
    "        util_minutes_capacity_total  = 0.0  # sum of available capacity minutes across post-warmup days\n",
    "        utilisation_by_day_post_warmup: List[dict] = []  # [{day, minutes_used, minutes_capacity, utilisation}]\n",
    "\n",
    "        # Patient-level records\n",
    "        patient_records: List[Dict[str, Any]] = []\n",
    "\n",
    "        # Sanity trace per kind\n",
    "        for k in queues:\n",
    "            tr(\"Param check\",\n",
    "               kind=k,\n",
    "               weight=priority_weights.get(k, 1.0),\n",
    "               mean_case_minutes_runtime=mean_case_minutes.get(k),\n",
    "               deadline_min=deadlines_min.get(k))\n",
    "\n",
    "        server = DailyServer(trace=tr)\n",
    "\n",
    "        # --- Main day loop ---\n",
    "        for day in range(horizon_days):\n",
    "            day_start_min = day * 1440\n",
    "            prev_day_start_min = (day - 1) * 1440 if day > 0 else None\n",
    "            tr(\"==== NEW DAY ====\", day=day, start_min=day_start_min, prev_day_start_min=prev_day_start_min)\n",
    "\n",
    "            # Capacity for the day\n",
    "            minutes_budget_today = self._minutes_capacity_for_day(sessions_by_dow, session_minutes, day)\n",
    "            tr(\"Daily capacity minutes\", day=day, minutes_budget_today=minutes_budget_today)\n",
    "\n",
    "            # Arrivals: include up to the last minute we could still be operating today\n",
    "            arrivals_cutoff = day_start_min + int(minutes_budget_today)\n",
    "            new_arrs = arrivals.pop_arrivals_up_to(arrivals_cutoff)\n",
    "            new_count = {k: len(v) for k, v in new_arrs.items()}\n",
    "            tr(\"Arrivals pulled\", day=day, cutoff=arrivals_cutoff, counts=new_count)\n",
    "\n",
    "            # Push arrivals into the queues\n",
    "            for kind, arr_list in new_arrs.items():\n",
    "                for arr in arr_list:\n",
    "                    pid = next(patient_id_counter)\n",
    "                    queues[kind].append((arr, pid))\n",
    "\n",
    "            # Serve once per day\n",
    "            (\n",
    "                minutes_left,            # minutes remaining unused\n",
    "                s_today,                 # served counts by kind\n",
    "                b_today,                 # served-in-breach counts by kind\n",
    "                w_today,                 # list of waits for served by kind\n",
    "                x_today,                 # list of excess waits for served by kind\n",
    "                day_patients,            # patient-level records for those served today\n",
    "                in_breach_start_day,     # snapshot counts at day start (already breached), by kind\n",
    "                new_breaches_day,        # snapshot counts of newly breached during the day, by kind\n",
    "            ) = server.serve_day(\n",
    "                queues=queues,\n",
    "                minutes_budget=minutes_budget_today,\n",
    "                day_start_min=day_start_min,\n",
    "                policy=policy,\n",
    "                deadlines_min=deadlines_min,\n",
    "                duration_sampler=duration_sampler,      # dict[str, ()->float]\n",
    "                mean_case_minutes=mean_case_minutes,\n",
    "                priority_weights=priority_weights,\n",
    "                prev_day_start_min=prev_day_start_min,\n",
    "            )\n",
    "\n",
    "            # Utilisation for today (independent of warmup; aggregated post-warmup below)\n",
    "            used_today = max(0.0, float(minutes_budget_today) - float(minutes_left))\n",
    "            tr(\"Utilisation today\",\n",
    "               day=day,\n",
    "               minutes_used=used_today,\n",
    "               minutes_capacity=float(minutes_budget_today),\n",
    "               minutes_left=float(minutes_left))\n",
    "\n",
    "            # Tag warm-up on served patients & record\n",
    "            for rec in day_patients:\n",
    "                rec[\"in_warmup\"] = (rec[\"service_start\"] is not None\n",
    "                                    and rec[\"service_start\"] < warmup_cut_min)\n",
    "            patient_records.extend(day_patients)\n",
    "            tr(\"Served today (by kind)\", day=day, served=s_today, served_in_breach=b_today)\n",
    "\n",
    "            # End-of-day breached backlog snapshot (by kind), after serving\n",
    "            in_breach_end_day = {k: 0 for k in queues}\n",
    "            day_end_min = day_start_min + 1440\n",
    "            for k, q in queues.items():\n",
    "                dline = deadlines_min[k]\n",
    "                for arr, _pid in q:\n",
    "                    if (day_end_min - arr) > dline:\n",
    "                        in_breach_end_day[k] += 1\n",
    "            tr(\"End-of-day breached backlog\", day=day, in_breach_end_day=in_breach_end_day)\n",
    "\n",
    "            # Accumulate KPIs after warm-up only\n",
    "            if day_start_min >= warmup_cut_min:\n",
    "                for k in s_today: served_counts[k]   += s_today[k]\n",
    "                for k in b_today: breached_counts[k] += b_today[k]\n",
    "                for k in w_today: waits_all[k]       += w_today[k]\n",
    "                for k in x_today: excess_all[k]      += x_today[k]\n",
    "                for k in in_breach_start_day: in_breach_start_tot[k] += in_breach_start_day[k]\n",
    "                for k in new_breaches_day:    new_breaches_tot[k]    += new_breaches_day[k]\n",
    "                for k in in_breach_end_day:   in_breach_end_tot[k]    += in_breach_end_day[k]\n",
    "\n",
    "                util_minutes_used_total     += used_today\n",
    "                util_minutes_capacity_total += float(minutes_budget_today)\n",
    "                utilisation_by_day_post_warmup.append({\n",
    "                    \"day\": day,\n",
    "                    \"minutes_used\": used_today,\n",
    "                    \"minutes_capacity\": float(minutes_budget_today),\n",
    "                    \"utilisation\": (used_today / float(minutes_budget_today)) if minutes_budget_today > 0 else 0.0,\n",
    "                })\n",
    "                tr(\"Post-warmup accumulation\",\n",
    "                   day=day,\n",
    "                   added_served=s_today,\n",
    "                   added_breached=b_today,\n",
    "                   added_in_breach_start=in_breach_start_day,\n",
    "                   added_new_breaches=new_breaches_day,\n",
    "                   added_in_breach_end=in_breach_end_day,\n",
    "                   util_used_total=util_minutes_used_total,\n",
    "                   util_cap_total=util_minutes_capacity_total)\n",
    "\n",
    "        # --- End-of-horizon snapshot for remaining (unserved) patients ---\n",
    "        waiting_count_final = sum(len(q) for q in queues.values())\n",
    "        waiting_minutes_final = sum(max(0, end_time_min - arr) for q in queues.values() for arr, _ in q)\n",
    "\n",
    "        breached_waiting_final = 0\n",
    "        breached_waiting_minutes_final = 0\n",
    "        for kind, q in queues.items():\n",
    "            deadline_min = deadlines_min[kind]\n",
    "            for arrival_min, pid in q:\n",
    "                wait   = end_time_min - arrival_min\n",
    "                excess = wait - deadline_min\n",
    "                breached_now = (wait > deadline_min) #actual breaches\n",
    "                if breached_now:\n",
    "                    breached_waiting_final += 1\n",
    "                    breached_waiting_minutes_final += int(max(0, excess))\n",
    "                #breach_time_val = (arrival_min + deadline_min) if breached_now else None\n",
    "                patient_records.append({\n",
    "                    \"id\": pid,\n",
    "                    \"kind\": kind,\n",
    "                    \"arrival\": int(arrival_min),\n",
    "                    \"deadline\": int(deadline_min),\n",
    "                    \"service_start\": None,\n",
    "                    \"service_end\": None,\n",
    "                    \"wait\": int(wait),\n",
    "                    \"excess\": int(max(0, excess)),\n",
    "                    \"breached\": bool(breached_now),\n",
    "                    #\"breach_time\": int(breach_time_val) if breach_time_val is not None else None,\n",
    "                    \"in_warmup\": arrival_min < warmup_cut_min,\n",
    "                    \"policy\": policy,\n",
    "                    \"breach_time\": int(arrival_min + deadline_min),\n",
    "                })\n",
    "        tr(\"End-of-horizon unserved snapshot\",\n",
    "           waiting_count_final=int(waiting_count_final),\n",
    "           waiting_minutes_final=int(waiting_minutes_final),\n",
    "           breached_waiting_final=int(breached_waiting_final),\n",
    "           breached_waiting_minutes_final=int(breached_waiting_minutes_final))\n",
    "\n",
    "        # --- Breach incidence from timestamps (post-warmup, within horizon) ---\n",
    "        breach_incidence_by_kind = defaultdict(int)\n",
    "        \n",
    "        for rec in patient_records:\n",
    "            bt = rec.get(\"breach_time\")\n",
    "            if bt is None or not (warmup_cut_min <= bt < end_time_min):\n",
    "                continue\n",
    "        \n",
    "            ss = rec.get(\"service_start\")\n",
    "            # Count only if the patient was *still waiting* at bt.\n",
    "            # Served: breach iff service_start > bt (strictly later than deadline).\n",
    "            # Unserved: breach iff end_of_horizon is after bt (already ensured by bt < end_time_min).\n",
    "            if (ss is None) or (ss > bt):\n",
    "                breach_incidence_by_kind[rec[\"kind\"]] += 1\n",
    "        \n",
    "        breach_incidence_total = int(sum(breach_incidence_by_kind.values()))   \n",
    "\n",
    "        # Utilisation summary (post-warmup days only)\n",
    "        utilisation_rate_post_warmup = (\n",
    "            float(util_minutes_used_total) / util_minutes_capacity_total\n",
    "            if util_minutes_capacity_total > 0 else 0.0\n",
    "        )\n",
    "        tr(\"Utilisation summary (post-warmup)\",\n",
    "           minutes_used_total=util_minutes_used_total,\n",
    "           minutes_capacity_total=util_minutes_capacity_total,\n",
    "           utilisation_rate=utilisation_rate_post_warmup)\n",
    "\n",
    "        # --- KPIs (with explanations) ---\n",
    "        \n",
    "        # Convenience totals\n",
    "        served_total              = int(sum(served_counts.values()))\n",
    "        breached_served_total     = int(sum(breached_counts.values()))                 # served while already past deadline\n",
    "        #breach_incidence_total    = int(sum(new_breaches_tot.values()))                # true events: first time a patient crosses deadline\n",
    "        \n",
    "        #  by-kind derived numbers\n",
    "        served_within_deadline_by_kind = {k: int(served_counts[k] - breached_counts[k]) for k in served_counts}\n",
    "        served_within_deadline_total   = int(sum(served_within_deadline_by_kind.values()))\n",
    "        \n",
    "        #  simple rates (guarded against divide-by-zero)\n",
    "        pct_within_deadline_overall = (served_within_deadline_total / served_total) if served_total > 0 else 0.0\n",
    "        pct_within_deadline_by_kind = {\n",
    "            k: (served_within_deadline_by_kind[k] / served_counts[k]) if served_counts[k] > 0 else 0.0\n",
    "            for k in served_counts\n",
    "        }\n",
    "        \n",
    "        kpis = {\n",
    "            \"policy\": policy,  # Service policy used (edd or pi)\n",
    "        \n",
    "            # -----------------------------\n",
    "            # FLOWS: Served patients (post-warmup; patient counts)\n",
    "            # -----------------------------\n",
    "            \"served_by_kind\": served_counts,                     # Patients served, by kind, after warm-up.\n",
    "            \"served_total\": served_total,                        # Total served after warm-up.\n",
    "        \n",
    "            # Performance view of served patients relative to the SLA at service start:\n",
    "            \"breached_served_by_kind\": breached_counts,          # Of the served, how many were already past deadline at service start (post-warmup).\n",
    "            \"breached_served_total\": breached_served_total,      # Total served-while-in-breach (post-warmup).\n",
    "        \n",
    "            # Compliance view for served patients:\n",
    "            \"served_within_deadline_by_kind\": served_within_deadline_by_kind,  # Served at/under deadline, by kind (post-warmup).\n",
    "            \"served_within_deadline_total\": served_within_deadline_total,      # Total served at/under deadline (post-warmup).\n",
    "            \"pct_within_deadline_by_kind\": pct_within_deadline_by_kind,        # Proportion of served within deadline, by kind (0..1).\n",
    "            \"pct_within_deadline_overall\": pct_within_deadline_overall,        # Overall proportion within deadline (0..1).\n",
    "        \n",
    "            # -----------------------------\n",
    "            # EVENTS: Breach incidence (official breach metric; post-warmup)\n",
    "            # -----------------------------\n",
    "            # These are *true events* counted once when a patient crosses the SLA (arrival_time + deadline).\n",
    "            \"breach_incidence_by_kind\": dict(breach_incidence_by_kind),        # Number of first-time breaches during the day, summed over post-warmup days.\n",
    "            \"breach_incidence_total\": breach_incidence_total,    # Total breach events across kinds (post-warmup).\n",
    "        \n",
    "            # Back-compat aliases so existing downstream code that expects 'breached_*' keeps working,\n",
    "            # but now points to the event-based definition:\n",
    "            \"breached_by_kind\": dict(breach_incidence_by_kind),                # ALIAS to incidence (preferred: use 'breach_incidence_by_kind').\n",
    "            \"breached_total\": breach_incidence_total,            # ALIAS to incidence (preferred: use 'breach_incidence_total').\n",
    "        \n",
    "            # -----------------------------\n",
    "            # STOCKS: End-of-horizon waiting (served + unserved context)\n",
    "            # -----------------------------\n",
    "            \"waiting_count_final\": int(waiting_count_final),     # Number of patients still waiting at the end of the horizon.\n",
    "            \"waiting_minutes_final\": int(waiting_minutes_final), # Sum of waits (end_time - arrival) for those still waiting.\n",
    "        \n",
    "            # Among the unserved at horizon end, breach status (using >= deadline):\n",
    "            \"breached_waiting_final\": int(breached_waiting_final),                   # Count of still-waiting who are in breach at horizon end.\n",
    "            \"breached_waiting_minutes_final\": int(breached_waiting_minutes_final),   # Sum of (wait - deadline, floored at 0) for those in breach.\n",
    "        \n",
    "            # -----------------------------\n",
    "            # EXPOSURE-DAY SNAPSHOTS (post-warmup sums)\n",
    "            # -----------------------------\n",
    "            # These are *exposures*, not people: each day contributes counts; a single patient can contribute on multiple days.\n",
    "            \"in_breach_start_total_by_kind\": in_breach_start_tot,  # At day start (08:00), already-in-breach; summed over post-warmup days.\n",
    "            \"in_breach_end_total_by_kind\": in_breach_end_tot,      # At day end (24:00), still waiting & in-breach; summed post-warmup.\n",
    "            # Kept for completeness/reference; these same counts feed 'breach_incidence_*' totals above:\n",
    "            \"new_breaches_total_by_kind\": new_breaches_tot,        # Exposure snapshots (not equal to incidence totals; last day events are not included)\n",
    "        \n",
    "            # -----------------------------\n",
    "            # WAIT DISTRIBUTIONS (served; post-warmup samples)\n",
    "            # -----------------------------\n",
    "            # Raw samples for downstream summaries/plots; minutes from arrival to service start.\n",
    "            \"waits_by_kind_minutes\": waits_all,                   # List[int] wait times for served patients, by kind (post-warmup).\n",
    "            \"excess_by_kind_minutes\": excess_all,                 # List[int] minutes over deadline at service start (0 if on time), by kind (post-warmup).\n",
    "        \n",
    "            # -----------------------------\n",
    "            # UTILISATION (post-warmup only)\n",
    "            # -----------------------------\n",
    "            \"util_minutes_used_total_post_warmup\": util_minutes_used_total,          # Sum of minutes actually used across post-warmup days.\n",
    "            \"util_minutes_capacity_total_post_warmup\": util_minutes_capacity_total,  # Sum of available minutes across post-warmup days.\n",
    "            \"utilisation_rate_post_warmup\": utilisation_rate_post_warmup,            # Used / Capacity over post-warmup days (0..1).\n",
    "            \"utilisation_by_day_post_warmup\": utilisation_by_day_post_warmup,        # Per-day series for plotting/diagnostics.\n",
    "        }\n",
    "\n",
    "\n",
    "        return {\"run_id\": run_id, \"seed_used\": base_seed, \"kpis\": kpis, \"patients\": patient_records}\n",
    "\n",
    "    # Convenience: batch runner that mirrors your loop\n",
    "    def run_many(self, n_reps: int) -> Tuple[List[Dict[str, Any]], Dict[int, List[dict]]]:\n",
    "        rows: List[Dict[str, Any]] = []\n",
    "        results_dict: Dict[int, List[dict]] = {}\n",
    "        for r in range(1, int(n_reps) + 1):\n",
    "            res = self.single_run(run_id=r)\n",
    "            rows.append({\"rep\": r, **res[\"kpis\"]})\n",
    "            results_dict[r] = res[\"patients\"]\n",
    "        return rows, results_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a02f6-a766-4cc6-9fc0-6366c906e2fc",
   "metadata": {},
   "source": [
    "# Get a clean copy of config ready for scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40972be-14b5-4c40-a7fa-2adc3bb81a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_cfg(cfg):\n",
    "    \"\"\"safe copy for scenario overrides.\"\"\"\n",
    "    return copy.deepcopy(cfg)\n",
    "\n",
    "def deep_update(dst: dict, src: dict) -> dict:\n",
    "    \"\"\"Recursive dict merge: dst <- src (modifies dst, returns dst).\"\"\"\n",
    "    for k, v in src.items():\n",
    "        if isinstance(v, dict) and isinstance(dst.get(k), dict):\n",
    "            deep_update(dst[k], v)\n",
    "        else:\n",
    "            dst[k] = v\n",
    "    return dst\n",
    "\n",
    "def apply_overrides(cfg, overrides: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Apply scenario overrides onto a cloned Config.\n",
    "    Supports both top-level attributes and nested dict merges.\n",
    "    \"\"\"\n",
    "    cfg2 = clone_cfg(cfg)\n",
    "    for k, v in (overrides or {}).items():\n",
    "        cur = getattr(cfg2, k, None)\n",
    "        if isinstance(cur, dict) and isinstance(v, dict):\n",
    "            deep_update(cur, v)  # in-place\n",
    "        else:\n",
    "            setattr(cfg2, k, v)\n",
    "    return cfg2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850d1c9-456c-443b-8d20-6229ae0d60b2",
   "metadata": {},
   "source": [
    "# Single run of base config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39485b-1b64-43c6-befb-18041fd4e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single(cfg, run_id: int = 1) -> dict:\n",
    "    sim = Simulation(cfg)\n",
    "    return sim.single_run(run_id=run_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e88f74-38c5-4d1b-b7c9-d7dcdb59ff2f",
   "metadata": {},
   "source": [
    "# Mulitple reps of base config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e4547b-364c-489a-80a0-50336bff46f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_reps(cfg, n_reps: int) -> Tuple[list, dict]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      rows: list of KPI dicts (one per rep; each has 'rep').\n",
    "      results_dict: {rep -> list[patient-record-dicts]}\n",
    "    \"\"\"\n",
    "    sim = Simulation(cfg)\n",
    "    rows: List[dict] = []\n",
    "    results_dict: Dict[int, list] = {}\n",
    "    for r in range(1, int(n_reps) + 1):\n",
    "        res = sim.single_run(run_id=r)\n",
    "        rows.append({\"rep\": r, **res[\"kpis\"]})\n",
    "        results_dict[r] = res[\"patients\"]\n",
    "    return rows, results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b31f7d-75aa-4564-ab79-17ff194a8a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_reps(rows):\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        df = pd.json_normalize(rows)\n",
    "        desc = df.select_dtypes(include=\"number\").describe()\n",
    "        means = df.mean(numeric_only=True)\n",
    "        stds  = df.std(numeric_only=True, ddof=1)\n",
    "        ci95  = 1.96 * stds / (len(df) ** 0.5)\n",
    "        summary = pd.DataFrame({\"mean\": means, \"std\": stds, \"ci95\": ci95})\n",
    "        return df, desc, summary\n",
    "    except Exception:\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb9f09d-780b-45a9-8fa5-e10c85e3ac02",
   "metadata": {},
   "source": [
    "# Build scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2121428c-3ebd-4d5b-bb2f-956c2bc173d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scenarios(\n",
    "    base_cfg,\n",
    "    scenarios: Dict[str, Dict[str, Any]],\n",
    "    n_reps: int,\n",
    "    *,\n",
    "    attach_patients_last_only: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs each scenario for n_reps.\n",
    "    Returns:\n",
    "      kpi_rows: list of dicts (columns: scenario, rep, KPIs...)\n",
    "      patients_by_scenario: {scenario -> {rep -> list[patient dicts]}}\n",
    "         If attach_patients_last_only=True, stores only the last replication's patients per scenario\n",
    "         under key rep == n_reps (compact for plotting later).\n",
    "    \"\"\"\n",
    "    kpi_rows: List[dict] = []\n",
    "    patients_by_scenario: Dict[str, Dict[int, list]] = {}\n",
    "\n",
    "    for scen_name, overrides in scenarios.items():\n",
    "        cfg_s = apply_overrides(base_cfg, overrides)\n",
    "        cfg_s.trace(\"Scenario config applied\", scenario=scen_name)\n",
    "        sim = Simulation(cfg_s)\n",
    "        patients_by_scenario[scen_name] = {}\n",
    "\n",
    "        for r in range(1, int(n_reps) + 1):\n",
    "            res = sim.single_run(run_id=r)\n",
    "            kpi_rows.append({\"scenario\": scen_name, \"rep\": r, **res[\"kpis\"]})\n",
    "\n",
    "            # store patients either every rep or only last rep\n",
    "            if not attach_patients_last_only or r == n_reps:\n",
    "                patients_by_scenario[scen_name][r] = res[\"patients\"]\n",
    "\n",
    "    return kpi_rows, patients_by_scenario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340b7587-f9b1-43bc-8450-f8598b383828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenarios_to_df(kpi_rows):\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        return pd.json_normalize(kpi_rows)\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c9be7-813a-4e26-b1da-e217c4ee020c",
   "metadata": {},
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e3ca2-e418-4a74-81c5-43b8cb542266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure base cfg is not mutated by overrides\n",
    "_base = copy.deepcopy(cfg)\n",
    "cfg_test = apply_overrides(cfg, {\"priority_weights\": {\"hip\": 9.99}})\n",
    "assert cfg.priority_weights[\"hip\"] == _base.priority_weights[\"hip\"], \"Base cfg was mutated!\"\n",
    "print(\"passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b91aa1c-fc30-496e-9b35-3082a642f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Instance ownership (i.e. no shared dicts)\n",
    "c1, c2 = Config(), Config()\n",
    "c1.priority_weights[\"hip\"] = 9.99\n",
    "assert c2.priority_weights[\"hip\"] != 9.99\n",
    "\n",
    "# 2) Stable seeds\n",
    "cfg = Config()\n",
    "s1 = cfg.seed_for(1, stream=\"arrivals\")\n",
    "s2 = cfg.seed_for(1, stream=\"arrivals\")\n",
    "assert s1 == s2  # should always hold across runs/processes\n",
    "\n",
    "# 3) Weekly capacity > 0 with defaults\n",
    "assert cfg.get_total_weekly_capacity_minutes() > 0\n",
    "print(\"all tests passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def12953-c82e-4ac3-af39-82a379a16fc9",
   "metadata": {},
   "source": [
    "# Example run scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9ed9b-e4bc-4ad1-b3d4-a64947af2bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Base config ---\n",
    "cfg = Config()\n",
    "cfg.horizon_days = 350\n",
    "cfg.warmup_days  = 35\n",
    "cfg.session_minutes = 240\n",
    "cfg.trauma_sessions_by_dow = {0:4,1:2,2:4,3:2,4:4,5:2,6:2}\n",
    "cfg.iat_mean_per_day = {\"hip\":360.0, \"shoulder\":1440.0, \"wrist\":480.0, \"ankle\":720.0}\n",
    "cfg.duration_params = {\n",
    "    \"hip\": {\"mean\":90.0, \"sd\":25.0, \"min\":30.0},\n",
    "    \"shoulder\":{\"mean\":75.0, \"sd\":22.5, \"min\":20.0},\n",
    "    \"wrist\":{\"mean\":60.0, \"sd\":18.0, \"min\":15.0},\n",
    "    \"ankle\":{\"mean\":55.0, \"sd\":27.0, \"min\":20.0},\n",
    "}\n",
    "cfg.breach_deadline_minutes = {\"hip\":36*60, \"shoulder\":14*24*60, \"wrist\":14*24*60, \"ankle\":14*24*60}\n",
    "cfg.service_policy = \"pi\"\n",
    "cfg.priority_weights = {\"hip\":2.0,\"shoulder\":1.0,\"wrist\":1.0,\"ankle\":1.0}\n",
    "cfg.base_seed = 1984\n",
    "cfg.TRACE = False\n",
    "\n",
    "# --- Single run ---\n",
    "res = run_single(cfg, run_id=1)\n",
    "print(\"KPIs:\", list(res[\"kpis\"].keys()))\n",
    "\n",
    "# # --- Reps ---\n",
    "# rows, results_dict = run_reps(cfg, n_reps=30)\n",
    "# df, desc, summary = summarize_reps(rows)\n",
    "# if df is not None:\n",
    "#     print(\"Replication summary:\"); print(desc)\n",
    "#     print(\"\\nKPI means ±95% CI:\"); print(summary)\n",
    "\n",
    "# --- Scenarios ---\n",
    "scenarios = {\n",
    "  \"baseline_pi\": {},\n",
    "  \"more_hips_priority\": {\"priority_weights\": {\"hip\": 3.0}},\n",
    "  \"edd_policy\": {\"service_policy\": \"edd\"},\n",
    "  \"more_capacity\": {\"trauma_sessions_by_dow\": {0:5,1:2,2:4,3:2,4:4,5:2,6:2}},\n",
    "}\n",
    "\n",
    "kpi_rows, patients_by_scen = run_scenarios(cfg, scenarios, n_reps=30, attach_patients_last_only=False)\n",
    "df_scen = scenarios_to_df(kpi_rows)\n",
    "if df_scen is not None:\n",
    "    print(\"\\nScenario KPI head:\")\n",
    "    print(df_scen.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3dfe66-75f0-4b28-83d1-a93dda314227",
   "metadata": {},
   "source": [
    "## Tests/validate scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f5925-ce87-46a5-b1ee-ebbc1cbb825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild patients df across scenarios/reps\n",
    "def patients_to_df(pby):\n",
    "    frames=[]\n",
    "    for scen, rep_map in pby.items():\n",
    "        for rep, plist in rep_map.items():\n",
    "            if not plist: continue\n",
    "            d=pd.DataFrame(plist)\n",
    "            if d.empty: continue\n",
    "            d[\"scenario\"]=scen; d[\"rep\"]=int(rep)\n",
    "            frames.append(d)\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "patients_df = patients_to_df(patients_by_scen).drop_duplicates(subset=[\"scenario\",\"rep\",\"id\"])\n",
    "\n",
    "warmup_cut_min = cfg.get_warmup_cut_min()\n",
    "end_time_min   = cfg.get_horizon_end_min()\n",
    "\n",
    "# ground-truth incidence from timestamps: warm-up window & still-waiting at bt\n",
    "pt_breach = (patients_df\n",
    "    .query(\"@warmup_cut_min <= breach_time < @end_time_min\")\n",
    "    .assign(_still_waiting=lambda d: d[\"service_start\"].isna() | (d[\"service_start\"] > d[\"breach_time\"]))\n",
    "    .query(\"_still_waiting\")\n",
    "    .groupby([\"scenario\",\"rep\"], observed=True)\n",
    "    .size().rename(\"breach_total_pt\").reset_index())\n",
    "\n",
    "df_kpis = pd.json_normalize(kpi_rows)\n",
    "cmp = (df_kpis[[\"scenario\",\"rep\",\"breach_incidence_total\"]]\n",
    "       .merge(pt_breach, on=[\"scenario\",\"rep\"], how=\"left\").fillna(0))\n",
    "\n",
    "# these should now match exactly\n",
    "assert (cmp[\"breach_incidence_total\"] - cmp[\"breach_total_pt\"]).abs().max() == 0\n",
    "\n",
    "# served totals should already match\n",
    "pt_served = (patients_df\n",
    "    .query(\"service_start.notna() and service_start >= @warmup_cut_min\")\n",
    "    .groupby([\"scenario\",\"rep\"], observed=True)\n",
    "    .size().rename(\"served_total_pt\").reset_index())\n",
    "\n",
    "cmp2 = (df_kpis[[\"scenario\",\"rep\",\"served_total\"]]\n",
    "        .merge(pt_served, on=[\"scenario\",\"rep\"], how=\"left\").fillna(0))\n",
    "assert (cmp2[\"served_total\"] - cmp2[\"served_total_pt\"]).abs().max() == 0\n",
    "\n",
    "print(\"passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9970fed0-512d-41d9-9bef-b3607009c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.validate()\n",
    "print(\"config ok\")\n",
    "\n",
    "# confirm each scenario actually changed what you think\n",
    "for name, ov in scenarios.items():\n",
    "    c = apply_overrides(cfg, ov)\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\" policy:\", c.get_policy(), \"weights:\", c.get_priority_weights()[\"hip\"],\n",
    "          \"Mon sessions:\", c.get_sessions_by_dow().get(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a508dba-4b02-4a14-bb41-f6cce8a8918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kpis = pd.json_normalize(kpi_rows)\n",
    "counts = (df_kpis.groupby(\"scenario\")[\"rep\"]\n",
    "                  .nunique()\n",
    "                  .rename(\"#reps\"))\n",
    "print(\"\\nrep counts per scenario:\\n\", counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6761d9-6306-4f08-afe7-5ea97bffc831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_kpis(kpi_rows):\n",
    "    df = pd.json_normalize(kpi_rows)\n",
    "    num_cols = [c for c in df.columns\n",
    "                if pd.api.types.is_numeric_dtype(df[c]) and c not in (\"rep\",)]\n",
    "    df_num = df[[\"scenario\",\"rep\"] + num_cols]\n",
    "\n",
    "    g = df_num.groupby(\"scenario\", as_index=False)\n",
    "    mean = g[num_cols].mean()\n",
    "    sd   = g[num_cols].std(ddof=1).rename(columns={c:f\"{c}_sd\" for c in num_cols})\n",
    "    n    = g.size().rename(columns={\"size\":\"n\"})\n",
    "    out  = mean.merge(sd, on=\"scenario\").merge(n, on=\"scenario\")\n",
    "    for c in num_cols:\n",
    "        out[f\"{c}_ci95\"] = 1.96 * (out[f\"{c}_sd\"] / np.sqrt(out[\"n\"]))\n",
    "    return df_num, out\n",
    "\n",
    "df_kpis_num, kpi_summary = summarize_kpis(kpi_rows)\n",
    "print(\"\\nKPI summary (mean ±95% CI) — selected columns:\")\n",
    "cols = [c for c in kpi_summary.columns if c.startswith((\"scenario\",\n",
    "    \"served_total\",\"breach_incidence_total\",\"pct_within_deadline_overall\",\n",
    "    \"utilisation_rate_post_warmup\")) or c==\"n\"]\n",
    "print(kpi_summary[cols].round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1293521-da65-4c5a-bcfc-c3d4951ab180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefer event-based breaches; fall back if needed\n",
    "def pick_breach_colnames(df):\n",
    "    if any(c.startswith(\"breach_incidence_by_kind.\") for c in df.columns):\n",
    "        pref = \"breach_incidence_by_kind.\"\n",
    "    elif any(c.startswith(\"breached_by_kind.\") for c in df.columns):\n",
    "        pref = \"breached_by_kind.\"\n",
    "    else:\n",
    "        pref = \"breached_served_by_kind.\"\n",
    "    return [c for c in df.columns if c.startswith(pref)], pref\n",
    "\n",
    "by_kind_cols, pref = pick_breach_colnames(df_kpis)\n",
    "served_cols = [c for c in df_kpis.columns if c.startswith(\"served_by_kind.\")]\n",
    "\n",
    "by_kind = (df_kpis[[\"scenario\"] + by_kind_cols + served_cols]\n",
    "           .groupby(\"scenario\", as_index=False).mean(numeric_only=True))\n",
    "\n",
    "print(\"\\nMean served_by_kind per scenario:\")\n",
    "print(by_kind[[\"scenario\"] + served_cols].round(1))\n",
    "\n",
    "print(f\"\\nMean {pref} per scenario:\")\n",
    "print(by_kind[[\"scenario\"] + by_kind_cols].round(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1b869-bcf8-4101-946b-dece0ca6b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that different reps used different seeds (proxy: served_total varies)\n",
    "vt = df_kpis_num.pivot_table(index=\"rep\", columns=\"scenario\", values=\"served_total\")\n",
    "print(\"\\nserved_total variance across reps (shouldn’t be all zeros):\")\n",
    "print(vt.var().round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124caf98-02af-4525-a5f5-d7d1eeff94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPI summary across reps (per scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b88a19a-b09c-4943-be61-ff8d8df56bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_kpis(kpi_rows):\n",
    "    df = pd.json_normalize(kpi_rows)\n",
    "\n",
    "    # numeric KPI columns (exclude the replication index)\n",
    "    num_cols = [c for c in df.columns\n",
    "                if pd.api.types.is_numeric_dtype(df[c]) and c not in (\"rep\",)]\n",
    "\n",
    "    # keep scenario/rep + numeric KPIs\n",
    "    df_num = df[[\"scenario\", \"rep\"] + num_cols].copy()\n",
    "\n",
    "    # group by scenario (keep 'scenario' as a column)\n",
    "    mean = df_num.groupby(\"scenario\", as_index=False)[num_cols].mean()\n",
    "    sd   = df_num.groupby(\"scenario\", as_index=False)[num_cols].std(ddof=1)\n",
    "    n    = df_num.groupby(\"scenario\", as_index=False).size().rename(columns={\"size\":\"n\"})\n",
    "\n",
    "    # add suffix to SD columns\n",
    "    sd = sd.rename(columns={c: f\"{c}_sd\" for c in sd.columns if c != \"scenario\"})\n",
    "\n",
    "    # merge on 'scenario'\n",
    "    out = mean.merge(sd, on=\"scenario\").merge(n, on=\"scenario\")\n",
    "\n",
    "    # 95% CI half-widths\n",
    "    for c in num_cols:\n",
    "        out[f\"{c}_ci95\"] = 1.96 * (out[f\"{c}_sd\"] / np.sqrt(out[\"n\"]))\n",
    "\n",
    "    return df, out\n",
    "\n",
    "\n",
    "df_kpis, kpi_summary = summarize_kpis(kpi_rows)\n",
    "print(\"KPI summary (mean ±95% CI):\")\n",
    "print(kpi_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa7056-0e81-4152-9105-f93cc6ddd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Patient level table across reps (per scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a28712-a345-4b5e-ac28-08975926064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patients_to_df(patients_by_scen):\n",
    "    frames = []\n",
    "    for scen, rep_map in patients_by_scen.items():\n",
    "        items = rep_map.items() if isinstance(rep_map, dict) else [(1, rep_map)]\n",
    "        for rep, plist in items:\n",
    "            if not plist:\n",
    "                continue\n",
    "            d = pd.DataFrame(plist)\n",
    "            if d.empty:\n",
    "                continue\n",
    "            d[\"scenario\"] = scen\n",
    "            d[\"rep\"] = int(rep)\n",
    "            frames.append(d)\n",
    "\n",
    "    # keep only frames that have at least one non-NA value\n",
    "    frames = [f for f in frames if (not f.empty) and f.notna().any().any()]\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "patients_df = patients_to_df(patients_by_scen)\n",
    "print(\"Patient rows:\", len(patients_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08e2aa0-591e-4024-acaf-9f44ea47951b",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979abb9-9d36-4d40-8d84-2a94faad5ecd",
   "metadata": {},
   "source": [
    "## KPI plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e865c7b-c71f-4672-a0a7-31c59b0f2d33",
   "metadata": {},
   "source": [
    "### Helpers - reshape and CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da563d8c-1d65-42f2-ac65-d08ac90439c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "\n",
    "def _is_scalar_series(s: pd.Series) -> bool:\n",
    "    # treat numbers/bools as scalar; exclude lists/dicts/arrays\n",
    "    return (pd.api.types.is_number(s.dtype) or pd.api.types.is_bool_dtype(s.dtype)) \\\n",
    "           and not s.apply(lambda x: isinstance(x, (dict, list, tuple, np.ndarray)) ).any()\n",
    "\n",
    "def melt_kpis(\n",
    "    df_scen: pd.DataFrame,\n",
    "    kpis: list[str] | None = None,\n",
    "    *,\n",
    "    id_cols: tuple[str, ...] = (\"scenario\", \"rep\"),\n",
    "    prefixes: tuple[str, ...] | None = None,\n",
    "    exclude_prefixes: tuple[str, ...] | None = (\"utilisation_by_day_\",),  # avoid per-day arrays by default\n",
    "    scenario_order: list[str] | None = None,\n",
    "    value_name: str = \"value\",\n",
    "    dropna: bool = True,\n",
    "    coerce_numeric: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Wide -> long for KPI plotting across reps.\n",
    "\n",
    "    - If `kpis` is None, auto-select scalar numeric columns (excluding id_cols and excluded prefixes).\n",
    "    - If `prefixes` is provided, include columns starting with any prefix (union with `kpis`).\n",
    "    - `exclude_prefixes` skips unwanted wide columns (e.g., per-day arrays).\n",
    "    \"\"\"\n",
    "    df = df_scen.copy()\n",
    "    present_ids = [c for c in id_cols if c in df.columns]\n",
    "\n",
    "    # auto-pick KPI columns that are scalar per cell\n",
    "    if kpis is None:\n",
    "        candidate_cols = [c for c in df.columns if c not in present_ids]\n",
    "        if exclude_prefixes:\n",
    "            candidate_cols = [c for c in candidate_cols if not any(c.startswith(p) for p in exclude_prefixes)]\n",
    "        # keep only scalar numeric/bool columns\n",
    "        scalar_cols = [c for c in candidate_cols if _is_scalar_series(df[c])]\n",
    "        kpis = scalar_cols\n",
    "\n",
    "    # add any requested prefixes\n",
    "    if prefixes:\n",
    "        pref_cols = [c for c in df.columns for p in prefixes if c.startswith(p)]\n",
    "        kpis = sorted(set(kpis).union(pref_cols))\n",
    "\n",
    "    # validate\n",
    "    missing = [c for c in kpis if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Requested KPI(s) not in DataFrame: {missing}\")\n",
    "\n",
    "    long = df.melt(id_vars=present_ids, value_vars=kpis, var_name=\"kpi\", value_name=value_name)\n",
    "\n",
    "    if coerce_numeric:\n",
    "        long[value_name] = pd.to_numeric(long[value_name], errors=\"coerce\")\n",
    "    if dropna:\n",
    "        long = long[long[value_name].notna()].copy()\n",
    "\n",
    "    if \"scenario\" in long.columns:\n",
    "        if scenario_order is None:\n",
    "            scenario_order = pd.unique(long[\"scenario\"])\n",
    "        long[\"scenario\"] = pd.Categorical(long[\"scenario\"], categories=list(scenario_order), ordered=True)\n",
    "\n",
    "    return long\n",
    "\n",
    "def ci95(\n",
    "    df_long: pd.DataFrame,\n",
    "    *,\n",
    "    val: str = \"value\",\n",
    "    group_cols: tuple[str, ...] = (\"scenario\", \"kpi\"),\n",
    "    z: float = 1.96,  # for large n; swap to t-multiplier if you prefer\n",
    "    observed: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    agg = (df_long.groupby(list(group_cols), as_index=False)\n",
    "                  .agg(mean=(val, \"mean\"), n=(val, \"size\"), sd=(val, \"std\")))\n",
    "    agg[\"se\"] = agg[\"sd\"] / np.sqrt(agg[\"n\"].clip(lower=1))\n",
    "    agg[\"lo\"] = agg[\"mean\"] - z * agg[\"se\"]\n",
    "    agg[\"hi\"] = agg[\"mean\"] + z * agg[\"se\"]\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04613f6-0d97-4c09-b4a4-36324129c71c",
   "metadata": {},
   "source": [
    "## examples of kpis with error bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56039468-acc9-45d0-99b5-b6ce2284a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scen = scenarios_to_df(kpi_rows)\n",
    "long = melt_kpis(\n",
    "    df_scen,\n",
    "    prefixes=(\"breach_incidence_total\", \"pct_within_deadline_overall\",\n",
    "              \"waiting_count_final\", \"utilisation_rate_post_warmup\")\n",
    ")\n",
    "summ = ci95(long)\n",
    "\n",
    "# Plot a single KPI (e.g., breach incidence) by scenario\n",
    "to_plot = summ[summ[\"kpi\"].eq(\"breach_incidence_total\")]\n",
    "px.bar(to_plot, x=\"scenario\", y=\"mean\", error_y=to_plot[\"hi\"]-to_plot[\"mean\"],\n",
    "       error_y_minus=to_plot[\"mean\"]-to_plot[\"lo\"],\n",
    "       title=\"Breach incidence (mean ±95% CI)\").show()\n",
    "\n",
    "# Or facet multiple KPIs at once\n",
    "px.bar(summ, x=\"scenario\", y=\"mean\", color=\"scenario\", facet_col=\"kpi\", facet_col_wrap=2,\n",
    "       error_y=summ[\"hi\"]-summ[\"mean\"], error_y_minus=summ[\"mean\"]-summ[\"lo\"],\n",
    "       title=\"KPIs by scenario (mean ±95% CI)\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d42851-14af-45fd-b826-bdcc1d843111",
   "metadata": {},
   "source": [
    "## 1. Throughput and breaches: served_by_kind and breach_incidence_by_kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae68b2e8-92aa-4db3-bdff-749dfff734a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMB_KINDS = (\"shoulder\", \"wrist\", \"ankle\")\n",
    "\n",
    "def make_grouped_long(df_scen: pd.DataFrame, amb_kinds=AMB_KINDS) -> pd.DataFrame:\n",
    "    df = df_scen.copy()\n",
    "\n",
    "    # Helper to fetch breach counts per kind (tries incidence first, then aliases)\n",
    "    breach_prefixes = (\n",
    "        \"breach_incidence_by_kind.\",      # preferred\n",
    "        \"breached_by_kind.\",              # alias you may have set to incidence\n",
    "        \"breached_served_by_kind.\",       # legacy served-while-in-breach\n",
    "    )\n",
    "    def get_breach_val(row, kind):\n",
    "        for pref in breach_prefixes:\n",
    "            key = f\"{pref}{kind}\"\n",
    "            if key in row and pd.notna(row[key]):\n",
    "                return row[key]\n",
    "        return np.nan\n",
    "\n",
    "    records = []\n",
    "    for _, r in df.iterrows():\n",
    "        # Served totals by group\n",
    "        hip_served = r.get(\"served_by_kind.hip\", np.nan)\n",
    "        amb_served = sum(r.get(f\"served_by_kind.{k}\", 0) for k in amb_kinds)\n",
    "\n",
    "        # Breach totals by group (using preferred metric available)\n",
    "        hip_breach = get_breach_val(r, \"hip\")\n",
    "        amb_breach = sum(get_breach_val(r, k) or 0 for k in amb_kinds)\n",
    "\n",
    "        for group, served, breached in (\n",
    "            (\"hip\", hip_served, hip_breach),\n",
    "            (\"ambulatory\", amb_served, amb_breach),\n",
    "        ):\n",
    "            records.append({\"scenario\": r[\"scenario\"], \"rep\": r[\"rep\"], \"group\": group,\n",
    "                            \"kpi\": \"served_total\", \"value\": served})\n",
    "            records.append({\"scenario\": r[\"scenario\"], \"rep\": r[\"rep\"], \"group\": group,\n",
    "                            \"kpi\": \"breached_total\", \"value\": breached})\n",
    "\n",
    "    long = pd.DataFrame.from_records(records)\n",
    "    return long\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07550561-e856-4743-980f-bb2e22217454",
   "metadata": {},
   "source": [
    "## Boxplot per scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb65106-b3d3-4e42-acba-2c272f0c0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "long = make_grouped_long(df_scen)\n",
    "\n",
    "fig = px.box(\n",
    "    long, x=\"scenario\", y=\"value\", color=\"group\",\n",
    "    facet_col=\"kpi\", facet_col_wrap=2, points=\"all\",\n",
    "    title=\"Throughput & Breaches by Group (per replication)\"\n",
    ")\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig.update_yaxes(matches=None)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb2bf2a-7299-42b3-84dd-1964cfca8df0",
   "metadata": {},
   "source": [
    "## Means with 95%CI pre scenario and patient type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8797c3a0-8002-4f7a-b61a-0ce98dbdb19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check that data is internally consistent\n",
    "# Which served_by_kind columns exist?\n",
    "served_cols = [c for c in df_scen.columns if c.startswith(\"served_by_kind.\")]\n",
    "print(\"served_by_kind cols:\", served_cols)\n",
    "\n",
    "# Check that per-kind sums match served_total per (scenario, rep)\n",
    "chk = df_scen[[\"scenario\",\"rep\",\"served_total\"] + served_cols].copy()\n",
    "chk[\"sum_kinds\"] = chk[served_cols].sum(axis=1, numeric_only=True)\n",
    "print((chk[\"served_total\"] - chk[\"sum_kinds\"]).describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b053bdd4-0e84-44ba-bb47-31460dff8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMB_KINDS = (\"shoulder\", \"wrist\", \"ankle\")\n",
    "WEEKDAYS = [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
    "\n",
    "def patients_to_df(pby):\n",
    "    frames=[]\n",
    "    for scen, rep_map in pby.items():\n",
    "        for rep, plist in rep_map.items():\n",
    "            if not plist: continue\n",
    "            d=pd.DataFrame(plist)\n",
    "            if d.empty: continue\n",
    "            d[\"scenario\"]=scen; d[\"rep\"]=int(rep)\n",
    "            frames.append(d)\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "def make_weekday_long(patients_by_scen, cfg) -> pd.DataFrame:\n",
    "    \"\"\"Return tidy per-(scenario,rep,weekday,group,kpi,value) for served & breach incidence.\"\"\"\n",
    "    pdf = patients_to_df(patients_by_scen).drop_duplicates(subset=[\"scenario\",\"rep\",\"id\"]).copy()\n",
    "    if pdf.empty:\n",
    "        return pd.DataFrame(columns=[\"scenario\",\"rep\",\"weekday\",\"weekday_name\",\"group\",\"kpi\",\"value\"])\n",
    "\n",
    "    # ensure breach_time exists (belt-and-braces)\n",
    "    if \"breach_time\" not in pdf.columns:\n",
    "        pdf[\"breach_time\"] = np.nan\n",
    "    need_bt = pdf[\"breach_time\"].isna() & pdf[\"breached\"].astype(\"boolean\", errors=\"ignore\").fillna(False)\n",
    "    pdf.loc[need_bt, \"breach_time\"] = pdf.loc[need_bt, \"arrival\"].astype(\"Int64\") + pdf.loc[need_bt, \"deadline\"].astype(\"Int64\")\n",
    "\n",
    "    warm = cfg.get_warmup_cut_min()\n",
    "    end  = cfg.get_horizon_end_min()\n",
    "\n",
    "    pdf[\"group\"] = np.where(pdf[\"kind\"].eq(\"hip\"), \"hip\", \"ambulatory\")\n",
    "\n",
    "    recs = []\n",
    "\n",
    "    # --- Served by weekday (post-warmup) ---\n",
    "    served = pdf[(pdf[\"service_start\"].notna()) & (pdf[\"service_start\"] >= warm)].copy()\n",
    "    served[\"weekday\"] = ((served[\"service_start\"] // 1440) % 7).astype(int)\n",
    "    g_served = (served\n",
    "                .groupby([\"scenario\",\"rep\",\"group\",\"weekday\"], observed=True)\n",
    "                .size().rename(\"value\").reset_index())\n",
    "    g_served[\"kpi\"] = \"served_total\"\n",
    "    recs.append(g_served)\n",
    "\n",
    "    # --- Breach incidence by weekday (post-warmup, still waiting at breach time) ---\n",
    "    breach = pdf[(pdf[\"breach_time\"].notna()) & (pdf[\"breach_time\"] >= warm) & (pdf[\"breach_time\"] < end)].copy()\n",
    "    # count only if still waiting at breach_time\n",
    "    breach[\"_still_waiting\"] = breach[\"service_start\"].isna() | (breach[\"service_start\"] > breach[\"breach_time\"])\n",
    "    breach = breach[breach[\"_still_waiting\"]].copy()\n",
    "    breach[\"weekday\"] = ((breach[\"breach_time\"] // 1440) % 7).astype(int)\n",
    "    g_breach = (breach\n",
    "                .groupby([\"scenario\",\"rep\",\"group\",\"weekday\"], observed=True)\n",
    "                .size().rename(\"value\").reset_index())\n",
    "    g_breach[\"kpi\"] = \"breached_total\"\n",
    "    recs.append(g_breach)\n",
    "\n",
    "    long = pd.concat(recs, ignore_index=True)\n",
    "    long[\"weekday_name\"] = pd.Categorical(long[\"weekday\"].map(dict(enumerate(WEEKDAYS))), categories=WEEKDAYS, ordered=True)\n",
    "    return long\n",
    "\n",
    "def ci95(df, val=\"value\", z=1.96, group_cols=(\"scenario\",\"group\",\"kpi\",\"weekday_name\")):\n",
    "    g = df.groupby(list(group_cols), as_index=False, observed=True)\n",
    "    out = g.agg(mean=(val,\"mean\"), n=(val,\"size\"), sd=(val,\"std\"))\n",
    "    out[\"se\"] = out[\"sd\"] / np.sqrt(out[\"n\"].clip(lower=1))\n",
    "    out[\"lo\"] = out[\"mean\"] - z*out[\"se\"]\n",
    "    out[\"hi\"] = out[\"mean\"] + z*out[\"se\"]\n",
    "    return out\n",
    "\n",
    "# --- Build weekday summary\n",
    "weekday_long = make_weekday_long(patients_by_scen, cfg)\n",
    "agg = ci95(weekday_long)\n",
    "\n",
    "# --- Plot: mean ±95% CI by weekday\n",
    "fig = px.line(\n",
    "    agg, x=\"weekday_name\", y=\"mean\", color=\"scenario\",\n",
    "    line_dash=\"group\",  # hip vs ambulatory\n",
    "    facet_col=\"kpi\", facet_col_wrap=2,\n",
    "    error_y=agg[\"hi\"]-agg[\"mean\"], error_y_minus=agg[\"mean\"]-agg[\"lo\"],\n",
    "    markers=True,\n",
    "    title=\"Served & Breach Incidence by Weekday (mean ±95% CI across reps)\"\n",
    ")\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig.update_xaxes(title=None)\n",
    "fig.update_yaxes(matches=None)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb52b10-72d1-4bd5-abe0-7bf25c97f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi = \"breached_total\"  # or \"served_total\"\n",
    "\n",
    "heat_df = (agg[agg[\"kpi\"].eq(kpi)]\n",
    "           .pivot_table(index=[\"scenario\",\"group\"],\n",
    "                        columns=\"weekday_name\",\n",
    "                        values=\"mean\",\n",
    "                        observed=False)\n",
    "           .reindex(columns=[\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]))\n",
    "\n",
    "# Make single y labels like \"baseline_pi • ambulatory\"\n",
    "ylabels = [f\"{s} • {g}\" for s, g in heat_df.index]\n",
    "\n",
    "fig = px.imshow(\n",
    "    heat_df.values,\n",
    "    x=heat_df.columns,\n",
    "    y=ylabels,\n",
    "    labels=dict(x=\"Weekday\", y=\"Scenario • Group\", color=f\"mean {kpi}\"),\n",
    "    aspect=\"auto\",\n",
    "    text_auto=True,\n",
    ")\n",
    "fig.update_layout(title=f\"{kpi}: mean by weekday (across reps)\")\n",
    "fig.update_coloraxes(colorscale=\"Viridis\", cmin=0, cmax=80)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d463d1-bab6-458e-be34-51a5bdfe9f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "WEEKDAYS = [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
    "\n",
    "for kpi in [\"served_total\", \"breached_total\"]:\n",
    "    dat = agg[agg[\"kpi\"].eq(kpi)].copy()\n",
    "\n",
    "    # lengths for error bars\n",
    "    dat[\"errp\"] = (dat[\"hi\"] - dat[\"mean\"]).fillna(0)\n",
    "    dat[\"errm\"] = (dat[\"mean\"] - dat[\"lo\"]).fillna(0)\n",
    "\n",
    "    # clip the lower CI at zero (counts can't be negative)\n",
    "    dat[\"errm\"] = np.minimum(dat[\"errm\"], dat[\"mean\"])  # ensures mean - errm >= 0\n",
    "\n",
    "    fig = px.bar(\n",
    "        dat,\n",
    "        x=\"weekday_name\", y=\"mean\", color=\"scenario\",\n",
    "        facet_row=\"group\",             # hip / ambulatory rows\n",
    "        barmode=\"group\",\n",
    "        error_y=\"errp\",                # <-- use column names\n",
    "        error_y_minus=\"errm\",\n",
    "        category_orders={\"weekday_name\": WEEKDAYS},\n",
    "        title=f\"{kpi}: mean ±95% CI by weekday\",\n",
    "    )\n",
    "\n",
    "    # presentation tweaks\n",
    "    fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "    fig.update_yaxes(matches=None, range=[0, None])     # never below zero\n",
    "    fig.update_traces(error_y=dict(symmetric=False, width=4, thickness=1))  # small caps\n",
    "\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f82ce-7bce-4bb1-b210-1d4a0f337f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b764c8bc-61c6-4df7-a6c9-e26d594443c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# A) KPIs per (scenario, rep)\n",
    "df_kpis = pd.json_normalize(kpi_rows)\n",
    "\n",
    "# pick the event-based breach total from KPIs\n",
    "kpi_breaches = (\n",
    "    \"breach_incidence_total\"\n",
    "    if \"breach_incidence_total\" in df_kpis.columns\n",
    "    else \"breached_total\"\n",
    ")\n",
    "\n",
    "# B) Build patients_df (one row per patient)\n",
    "def patients_to_df(patients_by_scen):\n",
    "    frames=[]\n",
    "    for scen, rep_map in patients_by_scen.items():\n",
    "        for rep, plist in rep_map.items():\n",
    "            if not plist: \n",
    "                continue\n",
    "            d = pd.DataFrame(plist)\n",
    "            if d.empty:\n",
    "                continue\n",
    "            d[\"scenario\"] = scen\n",
    "            d[\"rep\"] = int(rep)\n",
    "            frames.append(d)\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "patients_df = patients_to_df(patients_by_scen).drop_duplicates(\n",
    "    subset=[\"scenario\",\"rep\",\"id\"]\n",
    ").copy()\n",
    "\n",
    "# C) Ensure 'breach_time' exists where needed (belt-and-braces)\n",
    "if \"breach_time\" not in patients_df.columns:\n",
    "    patients_df[\"breach_time\"] = np.nan\n",
    "\n",
    "need_bt = patients_df[\"breach_time\"].isna() & patients_df[\"breached\"].astype(\"boolean\", errors=\"ignore\").fillna(False)\n",
    "patients_df.loc[need_bt, \"breach_time\"] = (\n",
    "    patients_df.loc[need_bt, \"arrival\"].astype(\"Int64\") + patients_df.loc[need_bt, \"deadline\"].astype(\"Int64\")\n",
    ")\n",
    "\n",
    "# D) Define analysis window + masks\n",
    "warmup_cut_min = cfg.get_warmup_cut_min()\n",
    "end_time_min   = cfg.get_horizon_end_min()\n",
    "\n",
    "served_mask  = patients_df[\"service_start\"].notna() & (patients_df[\"service_start\"] >= warmup_cut_min)\n",
    "\n",
    "# Breach incidence: within window AND still waiting at breach_time\n",
    "bt_in_window = patients_df[\"breach_time\"].notna() & (patients_df[\"breach_time\"] >= warmup_cut_min) & (patients_df[\"breach_time\"] < end_time_min)\n",
    "still_waiting_at_bt = patients_df[\"service_start\"].isna() | (patients_df[\"service_start\"] > patients_df[\"breach_time\"])\n",
    "breach_mask = bt_in_window & still_waiting_at_bt\n",
    "\n",
    "# E) Patient-derived totals per (scenario, rep)\n",
    "pt_served = (patients_df.loc[served_mask]\n",
    "             .groupby([\"scenario\",\"rep\"], observed=False)\n",
    "             .size().rename(\"served_total_pt\").reset_index())\n",
    "\n",
    "pt_breach = (patients_df.loc[breach_mask]\n",
    "             .groupby([\"scenario\",\"rep\"], observed=False)\n",
    "             .size().rename(\"breach_total_pt\").reset_index())\n",
    "\n",
    "# F) Compare to KPIs\n",
    "cmp = (df_kpis[[\"scenario\",\"rep\",kpi_breaches,\"served_total\"]]\n",
    "       .merge(pt_breach, on=[\"scenario\",\"rep\"], how=\"left\")\n",
    "       .merge(pt_served, on=[\"scenario\",\"rep\"], how=\"left\")\n",
    "       .fillna({\"breach_total_pt\":0, \"served_total_pt\":0}))\n",
    "\n",
    "cmp[\"delta_breach\"] = cmp[\"breach_total_pt\"] - cmp[kpi_breaches]\n",
    "cmp[\"delta_served\"] = cmp[\"served_total_pt\"] - cmp[\"served_total\"]\n",
    "\n",
    "print(cmp.sort_values([\"scenario\",\"rep\"]).head(12))\n",
    "print(\"\\nMax |delta| breach, served:\",\n",
    "      cmp[\"delta_breach\"].abs().max(), cmp[\"delta_served\"].abs().max())\n",
    "\n",
    "# strict invariants\n",
    "assert (cmp[\"delta_breach\"].abs().max() == 0), \"Incidence mismatch vs patients\"\n",
    "assert (cmp[\"delta_served\"].abs().max() == 0), \"Served mismatch vs patients\"\n",
    "print(\"KPI ↔ patients consistency checks passed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e9bf74-7f8e-4cef-9f3c-468a24c8244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breach incidence should be <= arrivals - served (post-warmup window-wise, roughly)\n",
    "# and should drop in higher-capacity scenarios:\n",
    "print(cmp.groupby(\"scenario\")[[\"breach_incidence_total\",\"served_total\"]].mean())\n",
    "\n",
    "# End-of-horizon backlog should be smallest in 'more_capacity'\n",
    "df_kpis = pd.json_normalize(kpi_rows)\n",
    "print(df_kpis.groupby(\"scenario\")[\"waiting_count_final\"].mean().sort_values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f10e9-036e-4dec-a96b-03ab34682b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fd5839b-d506-4a8c-9b9b-009f9347ed1a",
   "metadata": {},
   "source": [
    "## 2. Backlog: waiting_count_final vs breached_waiting_final  (end of horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7cbe4-91f4-4776-aff7-ed7d6fcb2ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpis = [\"waiting_count_final\",\"breached_waiting_final\"]\n",
    "long = melt_kpis(df_scen, kpis)\n",
    "px.box(long, x=\"scenario\", y=\"value\", color=\"scenario\",\n",
    "       facet_col=\"kpi\", facet_col_wrap=2,\n",
    "       title=\"End-of-horizon backlog & breached backlog\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf61a6e-44b1-4360-aab9-308a8e1ce2dd",
   "metadata": {},
   "source": [
    "# 3. Utilisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73b113d-7b67-49db-bb77-1cbd2ba41b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpis = [\"utilisation_rate_post_warmup\"]\n",
    "long = melt_kpis(df_scen, kpis)\n",
    "px.box(long, x=\"scenario\", y=\"value\", color=\"scenario\",\n",
    "       title=\"Utilisation rate (post-warmup)\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b742b9-7ff6-48ec-a997-f73ec7d633e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check splits are correct\n",
    "WEEKDAYS = [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
    "AMB_KINDS = (\"shoulder\",\"wrist\",\"ankle\")\n",
    "\n",
    "# capacity per (scenario,rep) from KPIs\n",
    "cap = (df_scen[[\"scenario\",\"rep\",\"util_minutes_capacity_total_post_warmup\",\n",
    "                \"utilisation_rate_post_warmup\"]]\n",
    "       .rename(columns={\"util_minutes_capacity_total_post_warmup\":\"capacity\"}))\n",
    "\n",
    "# patients table (one row per patient)\n",
    "def patients_to_df(pby):\n",
    "    frames=[]\n",
    "    for scen, rep_map in pby.items():\n",
    "        for rep, plist in rep_map.items():\n",
    "            if not plist: continue\n",
    "            d = pd.DataFrame(plist)\n",
    "            if d.empty: continue\n",
    "            d[\"scenario\"]=scen; d[\"rep\"]=int(rep)\n",
    "            frames.append(d)\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "patients_df = patients_to_df(patients_by_scen).drop_duplicates(subset=[\"scenario\",\"rep\",\"id\"]).copy()\n",
    "\n",
    "warm = cfg.get_warmup_cut_min()\n",
    "\n",
    "served = patients_df.query(\"service_start.notna() and service_start >= @warm\")\\\n",
    "                    .assign(group=lambda d: np.where(d[\"kind\"].eq(\"hip\"), \"hip\", \"ambulatory\"))\n",
    "\n",
    "used_by_group = (served.groupby([\"scenario\",\"rep\",\"group\"], observed=False)[\"duration\"]\n",
    "                 .sum().rename(\"minutes_used\").reset_index())\n",
    "\n",
    "# contributions = minutes_used_g / total capacity\n",
    "util_split = used_by_group.merge(cap, on=[\"scenario\",\"rep\"], how=\"left\")\\\n",
    "                          .assign(util_contribution=lambda d: d[\"minutes_used\"]/d[\"capacity\"])\n",
    "\n",
    "# check they sum to overall utilisation\n",
    "sum_by_rep = (util_split.pivot(index=[\"scenario\",\"rep\"], columns=\"group\", values=\"util_contribution\")\n",
    "                         .fillna(0.0))\n",
    "sum_by_rep[\"sum_groups\"] = sum_by_rep.sum(axis=1)\n",
    "\n",
    "recon = sum_by_rep.join(cap.set_index([\"scenario\",\"rep\"])[\"utilisation_rate_post_warmup\"])\n",
    "max_gap = (recon[\"sum_groups\"] - recon[\"utilisation_rate_post_warmup\"]).abs().max()\n",
    "print(f\"Max |sum(groups) - overall util| = {max_gap:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09c043-000a-41ab-b7b3-d3418824fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 0) Make sure you actually have per-rep patients (run_scenarios(..., attach_patients_last_only=False))\n",
    "def patients_to_df(patients_by_scen):\n",
    "    frames=[]\n",
    "    for scen, rep_map in patients_by_scen.items():\n",
    "        for rep, plist in rep_map.items():\n",
    "            if not plist: \n",
    "                continue\n",
    "            d = pd.DataFrame(plist)\n",
    "            if d.empty:\n",
    "                continue\n",
    "            d[\"scenario\"] = scen\n",
    "            d[\"rep\"] = int(rep)\n",
    "            frames.append(d)\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "patients_df = patients_to_df(patients_by_scen)\n",
    "\n",
    "# 1) KPI-side minutes used (post-warmup)\n",
    "df_scen = pd.json_normalize(kpi_rows)\n",
    "kpi_used = (df_scen[[\"scenario\",\"rep\",\"util_minutes_used_total_post_warmup\"]]\n",
    "            .rename(columns={\"util_minutes_used_total_post_warmup\":\"kpi_used\"}))\n",
    "\n",
    "# 2) Patient-side minutes used = sum of served durations post-warmup\n",
    "warmup_cut_min = cfg.get_warmup_cut_min()\n",
    "\n",
    "served_rows = patients_df.loc[\n",
    "    patients_df[\"service_start\"].notna() & (patients_df[\"service_start\"] >= warmup_cut_min)\n",
    "].copy()\n",
    "\n",
    "# ensure numeric duration (minutes)\n",
    "served_rows[\"duration\"] = served_rows[\"duration\"].astype(float)\n",
    "\n",
    "pt_used = (served_rows\n",
    "           .groupby([\"scenario\",\"rep\"], observed=False)[\"duration\"]\n",
    "           .sum()\n",
    "           .rename(\"pt_used\")\n",
    "           .reset_index())\n",
    "\n",
    "# 3) Merge and compare\n",
    "chk = (kpi_used.merge(pt_used, on=[\"scenario\",\"rep\"], how=\"outer\")  # outer to spot gaps\n",
    "               .fillna(0))                                         # 0 minutes for missing reps\n",
    "\n",
    "# Use a tolerance (floating-point arithmetic)\n",
    "tol = 1e-6\n",
    "diff = (chk[\"pt_used\"] - chk[\"kpi_used\"]).abs()\n",
    "print(\"max abs diff (min):\", diff.max())\n",
    "\n",
    "# If you want a hard assertion:\n",
    "assert np.allclose(chk[\"pt_used\"], chk[\"kpi_used\"], atol=tol), \"Mismatch between patient-sum and KPI minutes used.\"\n",
    "\n",
    "print(\"passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd0605f-74c3-4385-b1ba-d9f29aa2f0ad",
   "metadata": {},
   "source": [
    "# 5. Surgical types served "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe0c980-92b2-4304-848c-f5fce2c62a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "served_cols = [c for c in df_scen.columns if c.startswith(\"served_by_kind.\")]\n",
    "breached_cols = [c for c in df_scen.columns if c.startswith(\"breached_by_kind.\")]\n",
    "\n",
    "long_srv = melt_kpis(df_scen, served_cols)\n",
    "long_srv[\"kpi\"] = long_srv[\"kpi\"].str.replace(\"served_by_kind.\",\"\")\n",
    "\n",
    "px.box(long_srv, x=\"scenario\", y=\"value\", color=\"scenario\",\n",
    "       facet_col=\"kpi\", facet_col_wrap=4,\n",
    "       title=\"Served by kind (per replication)\").show()\n",
    "\n",
    "long_br = melt_kpis(df_scen, breached_cols)\n",
    "long_br[\"kpi\"] = long_br[\"kpi\"].str.replace(\"breached_by_kind.\",\"\")\n",
    "px.box(long_br, x=\"scenario\", y=\"value\", color=\"scenario\",\n",
    "       facet_col=\"kpi\", facet_col_wrap=4,\n",
    "       title=\"Breached (served) by kind (per replication)\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3defd9-8ecf-4901-93f3-b265883df827",
   "metadata": {},
   "source": [
    "## 5. Time series utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047293ed-0e21-408b-a6e4-a33c0a41312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode the per-rep list of dicts into rows\n",
    "u = (df_scen[[\"scenario\",\"rep\",\"utilisation_by_day_post_warmup\"]]\n",
    "       .explode(\"utilisation_by_day_post_warmup\")\n",
    "       .dropna())\n",
    "u = pd.concat([u.drop(columns=[\"utilisation_by_day_post_warmup\"]),\n",
    "               u[\"utilisation_by_day_post_warmup\"].apply(pd.Series)], axis=1)\n",
    "\n",
    "# mean & CI by day & scenario\n",
    "g = (u.groupby([\"scenario\",\"day\"], as_index=False)\n",
    "       .agg(mean=(\"utilisation\",\"mean\"), n=(\"utilisation\",\"size\"), se=(\"utilisation\",\"sem\")))\n",
    "g[\"lo\"] = g[\"mean\"] - 1.96*g[\"se\"]\n",
    "g[\"hi\"] = g[\"mean\"] + 1.96*g[\"se\"]\n",
    "\n",
    "fig = px.line(g, x=\"day\", y=\"mean\", color=\"scenario\",\n",
    "              title=\"Utilisation over time (mean ±95% CI)\")\n",
    "# add ribbons\n",
    "for scen in g[\"scenario\"].unique():\n",
    "    sub = g[g[\"scenario\"]==scen]\n",
    "    fig.add_traces(px.scatter(sub, x=\"day\", y=\"hi\").update_traces(visible=False).data)\n",
    "    fig.add_traces(px.scatter(sub, x=\"day\", y=\"lo\").update_traces(visible=False).data)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e4ccd-316b-4043-b1ae-dd6f2b730ebc",
   "metadata": {},
   "source": [
    "## 6. Sctterplot breach by service\n",
    "\n",
    "'More capacity' scenario serves more with fewer breaches. Overall runs with higher throughput have fewer breaches (near saturation behaviour so small slack collapses waits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a51ef9-cd4d-493c-8e73-7be9690c6b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df_scen, x=\"served_total\", y=\"breach_incidence_total\",\n",
    "           color=\"scenario\", hover_data=[\"rep\"],\n",
    "           trendline=\"ols\", trendline_scope=\"trace\",\n",
    "           title=\"Throughput vs Breached (each dot = replication)\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac600200-3f54-41fb-873b-711c8912a9ca",
   "metadata": {},
   "source": [
    "## Breaches by weekday (counts and per-day average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd13d95-a07b-4493-a382-6b67f6923705",
   "metadata": {},
   "source": [
    "## Utilislation by weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690d240-f993-48b9-9b71-25cbd9035ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WEEKDAYS = [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
    "\n",
    "def plot_util_by_weekday(df_scen: pd.DataFrame, *, show_points=True, ci=0.95, pad=0.01):\n",
    "    \"\"\"\n",
    "    Plot utilisation by weekday (post-warmup) with mean ±95% CI per scenario.\n",
    "\n",
    "    df_scen: pd.DataFrame from pd.json_normalize(kpi_rows)\n",
    "             must contain ['scenario','rep','utilisation_by_day_post_warmup'] where each entry\n",
    "             is a list of dicts: {'day', 'minutes_used', 'minutes_capacity', 'utilisation'}.\n",
    "\n",
    "    show_points: overlay per-rep daily points to see spread.\n",
    "    ci: confidence level for error bars (use 0.95).\n",
    "    pad: extra padding for y-axis (fraction of 1.0).\n",
    "    \"\"\"\n",
    "    # ---- 1) Flatten per-day utilisation into a tidy frame\n",
    "    util_rows = []\n",
    "    for _, r in df_scen[[\"scenario\",\"rep\",\"utilisation_by_day_post_warmup\"]].iterrows():\n",
    "        daylist = r[\"utilisation_by_day_post_warmup\"]\n",
    "        if not isinstance(daylist, list):\n",
    "            continue\n",
    "        rep = int(r[\"rep\"])\n",
    "        scen = r[\"scenario\"]\n",
    "        for d in daylist:\n",
    "            day_idx = int(d[\"day\"])\n",
    "            util_rows.append({\n",
    "                \"scenario\": scen,\n",
    "                \"rep\": rep,\n",
    "                \"day\": day_idx,\n",
    "                \"weekday_idx\": day_idx % 7,\n",
    "                \"weekday_name\": WEEKDAYS[day_idx % 7],\n",
    "                \"utilisation\": float(d[\"utilisation\"]),\n",
    "            })\n",
    "    util_by_day = pd.DataFrame(util_rows)\n",
    "    if util_by_day.empty:\n",
    "        raise ValueError(\"No per-day utilisation data found in 'utilisation_by_day_post_warmup'.\")\n",
    "\n",
    "    # ---- 2) Aggregate mean ± 95% CI (normal approx; fine for n≈30)\n",
    "    g = (util_by_day\n",
    "         .groupby([\"scenario\",\"weekday_name\"], observed=False)[\"utilisation\"])\n",
    "    agg = (g.agg(mean=\"mean\", n=\"size\", sd=\"std\").reset_index())\n",
    "    z = 1.96 if abs(ci - 0.95) < 1e-9 else 1.96  # keep simple; swap to t if you want small-n correction\n",
    "    agg[\"se\"]  = agg[\"sd\"] / np.sqrt(agg[\"n\"].clip(lower=1))\n",
    "    agg[\"lo\"]  = (agg[\"mean\"] - z * agg[\"se\"]).clip(lower=0)\n",
    "    agg[\"hi\"]  = (agg[\"mean\"] + z * agg[\"se\"]).clip(upper=1)\n",
    "    agg[\"errp\"] = (agg[\"hi\"] - agg[\"mean\"]).fillna(0)\n",
    "    agg[\"errm\"] = (agg[\"mean\"] - agg[\"lo\"]).fillna(0)\n",
    "\n",
    "    # Stable weekday order\n",
    "    agg[\"weekday_name\"] = pd.Categorical(agg[\"weekday_name\"], categories=WEEKDAYS, ordered=True)\n",
    "    util_by_day[\"weekday_name\"] = pd.Categorical(util_by_day[\"weekday_name\"], categories=WEEKDAYS, ordered=True)\n",
    "\n",
    "    # ---- 3) Auto-zoom Y so you can actually see the bars\n",
    "    ymin = float(max(0.0, agg[\"lo\"].min() - pad))\n",
    "    ymax = float(min(1.0, agg[\"hi\"].max() + pad))\n",
    "    if ymax - ymin < 0.02:          # ensure at least a 2% window\n",
    "        mid = (ymin + ymax) / 2\n",
    "        ymin = max(0.0, mid - 0.01)\n",
    "        ymax = min(1.0, mid + 0.01)\n",
    "\n",
    "    # ---- 4) Plot: mean line with CI error bars; optional raw points\n",
    "    fig = px.line(\n",
    "        agg,\n",
    "        x=\"weekday_name\", y=\"mean\", color=\"scenario\", markers=True,\n",
    "        error_y=\"errp\", error_y_minus=\"errm\",\n",
    "        title=\"Utilisation by weekday (mean ±95% CI, post-warmup)\",\n",
    "        labels={\"weekday_name\":\"Weekday\",\"mean\":\"Utilisation\"},\n",
    "    )\n",
    "    fig.update_yaxes(range=[ymin, ymax], tickformat=\".0%\")\n",
    "\n",
    "    if show_points:\n",
    "        pts = px.scatter(\n",
    "            util_by_day, x=\"weekday_name\", y=\"utilisation\", color=\"scenario\",\n",
    "            opacity=0.4, hover_data=[\"rep\"],\n",
    "        )\n",
    "        for tr in pts.data:\n",
    "            tr.showlegend = False\n",
    "            fig.add_trace(tr)\n",
    "\n",
    "    fig.show()\n",
    "    return agg, util_by_day\n",
    "\n",
    "# --- call it\n",
    "# df_scen = pd.json_normalize(kpi_rows)\n",
    "agg_util, util_by_day = plot_util_by_weekday(df_scen, show_points=True, ci=0.95, pad=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd1624-d10d-4b25-8ca8-2afff08bbd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    " px.box(util_by_day, x=\"weekday_name\", y=\"utilisation\", color=\"scenario\",\n",
    "       category_orders={\"weekday_name\": WEEKDAYS},\n",
    "       title=\"Utilisation by weekday (per-rep distribution)\",\n",
    "       labels={\"weekday_name\":\"Weekday\",\"utilisation\":\"Utilisation\"}) \\\n",
    "  .update_yaxes(tickformat=\".0%\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d278fc-4d32-48f4-8254-d3df4fce39ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qucik check on idle capacity - seems counterintuitive\n",
    "\n",
    "WEEKDAYS = [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
    "\n",
    "def build_util_by_day(df_scen: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flatten df_scen['utilisation_by_day_post_warmup'] into a tidy table:\n",
    "    one row per (scenario, rep, day).\n",
    "    \"\"\"\n",
    "    required = {\"scenario\",\"rep\",\"utilisation_by_day_post_warmup\"}\n",
    "    missing = required - set(df_scen.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"df_scen is missing: {sorted(missing)}\")\n",
    "\n",
    "    rows = []\n",
    "    for _, r in df_scen[[\"scenario\",\"rep\",\"utilisation_by_day_post_warmup\"]].iterrows():\n",
    "        lst = r[\"utilisation_by_day_post_warmup\"]\n",
    "        if not isinstance(lst, list):\n",
    "            # tolerate None/NaN gracefully\n",
    "            continue\n",
    "        scen = r[\"scenario\"]\n",
    "        rep  = int(r[\"rep\"])\n",
    "        for d in lst:\n",
    "            # tolerate missing keys with .get(..., 0.0)\n",
    "            day_idx   = int(d.get(\"day\", 0))\n",
    "            used_min  = float(d.get(\"minutes_used\", 0.0))\n",
    "            cap_min   = float(d.get(\"minutes_capacity\", 0.0))\n",
    "            util      = float(d.get(\"utilisation\", (used_min / cap_min) if cap_min > 0 else 0.0))\n",
    "            rows.append({\n",
    "                \"scenario\": scen,\n",
    "                \"rep\": rep,\n",
    "                \"day\": day_idx,\n",
    "                \"weekday_idx\": day_idx % 7,\n",
    "                \"weekday_name\": WEEKDAYS[day_idx % 7],\n",
    "                \"minutes_used\": used_min,\n",
    "                \"minutes_capacity\": cap_min,\n",
    "                \"utilisation\": util,\n",
    "            })\n",
    "    out = pd.DataFrame(rows)\n",
    "    if out.empty:\n",
    "        raise ValueError(\"No per-day utilisation data found. Did the KPIs include 'utilisation_by_day_post_warmup'?\")\n",
    "    # stable weekday order\n",
    "    out[\"weekday_name\"] = pd.Categorical(out[\"weekday_name\"], categories=WEEKDAYS, ordered=True)\n",
    "    return out\n",
    "\n",
    "# --- 1) Build util_by_day from your KPI rows\n",
    "# df_scen = pd.json_normalize(kpi_rows)  # you likely already have this\n",
    "util_by_day = build_util_by_day(df_scen)\n",
    "\n",
    "# --- 2) Quick checks (SAFE versions) -----------------------------\n",
    "\n",
    "# 2a) Any days with capacity but zero work?\n",
    "idle = util_by_day.query(\"minutes_capacity > 0 and minutes_used == 0\")\n",
    "idle_counts = (idle.groupby(\"scenario\", observed=False)[\"weekday_name\"]\n",
    "                   .value_counts()\n",
    "                   .rename(\"idle_days\")\n",
    "                   .reset_index())\n",
    "print(\"Idle operating days (count) by scenario & weekday:\\n\", idle_counts.head(20), \"\\n\")\n",
    "\n",
    "# 2b) Share of idle operating days per scenario\n",
    "share_idle = (util_by_day.assign(\n",
    "                    idle=(util_by_day[\"minutes_capacity\"]>0) & (util_by_day[\"minutes_used\"]==0))\n",
    "              .groupby(\"scenario\", observed=False)[\"idle\"]\n",
    "              .mean()\n",
    "              .rename(\"share_idle_days\"))\n",
    "print(\"Share of operating days that were idle (per scenario):\\n\", share_idle, \"\\n\")\n",
    "\n",
    "# 2c) Sanity: capacity==0 days (should usually be 0 with your DOW plan)\n",
    "cap0 = (util_by_day[\"minutes_capacity\"] == 0).sum()\n",
    "print(\"Days with zero capacity recorded:\", int(cap0), \"\\n\")\n",
    "\n",
    "# 2d) Show a few idle examples to eyeball\n",
    "examples = idle.sort_values([\"scenario\",\"rep\",\"day\"]).head(10)\n",
    "print(\"Example idle days:\\n\", examples[[\"scenario\",\"rep\",\"day\",\"weekday_name\",\"minutes_capacity\",\"minutes_used\",\"utilisation\"]], \"\\n\")\n",
    "\n",
    "# --- 3) Optional: weekly roll-up (smoother picture) -------------\n",
    "wb = (util_by_day.assign(week=lambda d: (d[\"day\"] // 7).astype(int))\n",
    "      .groupby([\"scenario\",\"rep\",\"week\"], observed=False)\n",
    "      .agg(minutes_used=(\"minutes_used\",\"sum\"),\n",
    "           minutes_capacity=(\"minutes_capacity\",\"sum\"))\n",
    "      .reset_index())\n",
    "wb[\"weekly_util\"] = np.where(wb[\"minutes_capacity\"]>0,\n",
    "                             wb[\"minutes_used\"]/wb[\"minutes_capacity\"], 0.0)\n",
    "\n",
    "# Per-scenario weekly mean ±95% CI\n",
    "wk = (wb.groupby([\"scenario\",\"week\"], observed=False)[\"weekly_util\"]\n",
    "        .agg(mean=\"mean\", n=\"size\", sd=\"std\").reset_index())\n",
    "wk[\"se\"] = wk[\"sd\"] / np.sqrt(wk[\"n\"].clip(lower=1))\n",
    "wk[\"lo\"] = (wk[\"mean\"] - 1.96*wk[\"se\"]).clip(lower=0)\n",
    "wk[\"hi\"] = (wk[\"mean\"] + 1.96*wk[\"se\"]).clip(upper=1)\n",
    "\n",
    "print(\"Weekly utilisation (first few rows):\\n\", wk.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df531fa-351e-4950-91b8-f833de78dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# days with capacity but no work (per scenario, weekday)\n",
    "idle = util_by_day.query(\"minutes_capacity > 0 and minutes_used == 0\")\n",
    "idle.groupby(\"scenario\")[\"weekday_name\"].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e439d-1c7a-4ec4-9b39-d331e9446b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# share of idle operating days\n",
    "(util_by_day.assign(idle=lambda d: (d.minutes_capacity>0) & (d.minutes_used==0))\n",
    " .groupby(\"scenario\")[\"idle\"].mean().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c6415d-0e9c-44a5-b8b0-a9dbd65d30c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_breached_served_per_day(\n",
    "    df_in: pd.DataFrame,\n",
    "    *,\n",
    "    warmup_days: int | None = 28,\n",
    "    kind_to_cat: dict | None = None,   # e.g. {\"hip\":\"hip\",\"shoulder\":\"amb\",\"wrist\":\"amb\",\"ankle\":\"amb\"}\n",
    "    rolling: int | None = 7,           # set None to disable smoothing\n",
    "    scenario_col: str | None = \"scenario\",\n",
    "    facet: bool = True,\n",
    "    title: str = \"Breached cases operated per day\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots # of served-in-breach per day (optionally 7-day rolling), split hip vs amb,\n",
    "    optionally faceted by scenario. Returns (dataframe_used, figure).\n",
    "    \"\"\"\n",
    "\n",
    "    df = df_in.copy()\n",
    "\n",
    "    # --- Served only & derive service_day ---\n",
    "    if \"service_start\" not in df.columns:\n",
    "        raise ValueError(\"df_in must contain 'service_start'.\")\n",
    "    df = df[df[\"service_start\"].notna()].copy()\n",
    "    df[\"service_day\"] = (df[\"service_start\"] // 1440).astype(int)\n",
    "\n",
    "    # --- Warmup filter (post-warmup only) ---\n",
    "    if warmup_days is not None:\n",
    "        df = df[df[\"service_start\"] >= int(warmup_days) * 1440]\n",
    "\n",
    "    # --- Breached-at-service flag (>= to match your model) ---\n",
    "    need = {\"arrival\",\"deadline\"}\n",
    "    if \"breached\" not in df.columns:\n",
    "        if not need.issubset(df.columns):\n",
    "            missing = sorted(need - set(df.columns))\n",
    "            raise ValueError(f\"Need columns {missing} to infer 'breached'.\")\n",
    "        df[\"breached\"] = (df[\"service_start\"] - df[\"arrival\"]) >= df[\"deadline\"]\n",
    "\n",
    "    # --- Category column hip vs amb ---\n",
    "    if \"kind\" not in df.columns:\n",
    "        raise ValueError(\"df_in must contain 'kind' to split hip vs amb.\")\n",
    "    if kind_to_cat is not None:\n",
    "        df[\"cat\"] = df[\"kind\"].map(kind_to_cat).fillna(\"other\")\n",
    "    else:\n",
    "        df[\"cat\"] = np.where(df[\"kind\"].eq(\"hip\"), \"hip\", \"amb\")\n",
    "\n",
    "    # --- Grouping keys ---\n",
    "    group_cols = [\"service_day\",\"cat\"]\n",
    "    has_scen = bool(scenario_col) and (scenario_col in df.columns)\n",
    "    if has_scen:\n",
    "        group_cols.append(scenario_col)\n",
    "\n",
    "    # --- Daily counts of breached-served ---\n",
    "    by_service = (\n",
    "        df.loc[df[\"breached\"]]\n",
    "          .groupby(group_cols, as_index=False, observed=False)\n",
    "          .size()\n",
    "          .rename(columns={\"size\":\"breaches\"})\n",
    "          .sort_values(group_cols)\n",
    "    )\n",
    "\n",
    "    # --- Optional rolling mean ---\n",
    "    ycol = \"breaches\"\n",
    "    if rolling and rolling > 1:\n",
    "        gb_cols = [c for c in group_cols if c != \"service_day\"]\n",
    "        by_service[\"breaches_rm\"] = (\n",
    "            by_service.groupby(gb_cols, group_keys=False)[\"breaches\"]\n",
    "                      .apply(lambda s: s.rolling(int(rolling), min_periods=1).mean())\n",
    "        ).values\n",
    "        ycol = \"breaches_rm\"\n",
    "\n",
    "    # --- Plot ---\n",
    "    ttl = f\"{title}{f' (rolling={rolling})' if rolling and rolling>1 else ''}\"\n",
    "    if has_scen and facet:\n",
    "        fig = px.line(by_service, x=\"service_day\", y=ycol,\n",
    "                      color=\"cat\",\n",
    "                      facet_col=scenario_col, facet_col_wrap=3,\n",
    "                      markers=True,\n",
    "                      title=ttl,\n",
    "                      labels={\"service_day\":\"Day\", ycol:\"# breached served\", \"cat\":\"Group\", scenario_col:\"Scenario\"})\n",
    "        # Clean facet labels\n",
    "        fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "    elif has_scen:\n",
    "        fig = px.line(by_service, x=\"service_day\", y=ycol,\n",
    "                      color=scenario_col, line_dash=\"cat\",\n",
    "                      markers=True,\n",
    "                      title=ttl,\n",
    "                      labels={\"service_day\":\"Day\", ycol:\"# breached served\", \"cat\":\"Group\", scenario_col:\"Scenario\"})\n",
    "    else:\n",
    "        fig = px.line(by_service, x=\"service_day\", y=ycol,\n",
    "                      color=\"cat\", markers=True,\n",
    "                      title=ttl,\n",
    "                      labels={\"service_day\":\"Day\", ycol:\"# breached served\", \"cat\":\"Group\"})\n",
    "\n",
    "    fig.update_traces(mode=\"lines+markers\")\n",
    "    return by_service, fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f8a1eb-566c-4216-bbd5-f9a8c61fce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def patients_to_df(patients_by_scen: dict) -> pd.DataFrame:\n",
    "    \"\"\"Flatten {scenario: {rep: [patient_dict,...]}} -> DataFrame.\"\"\"\n",
    "    frames = []\n",
    "    for scen, rep_map in (patients_by_scen or {}).items():\n",
    "        for rep, plist in (rep_map or {}).items():\n",
    "            if not plist: \n",
    "                continue\n",
    "            d = pd.DataFrame(plist)\n",
    "            if d.empty:\n",
    "                continue\n",
    "            d[\"scenario\"] = scen\n",
    "            d[\"rep\"] = int(rep)\n",
    "            frames.append(d)\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "# Rebuild df_scenarios (requires that you've already run run_scenarios(...) earlier)\n",
    "df_scenarios = patients_to_df(patients_by_scen)\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"df_scenarios shape:\", df_scenarios.shape)\n",
    "print(\"columns:\", sorted(df_scenarios.columns)[:15], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b37f3a7-94ec-44c7-b052-86c34add480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kind_to_cat = {\"hip\":\"hip\",\"shoulder\":\"amb\",\"wrist\":\"amb\",\"ankle\":\"amb\"}\n",
    "_ = plot_breached_served_per_day(\n",
    "    df_scenarios,\n",
    "    warmup_days=cfg.warmup_days,\n",
    "    kind_to_cat=kind_to_cat,\n",
    "    rolling=7,\n",
    "    scenario_col=\"scenario\",\n",
    "    facet=True  # False to overlay scenarios\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b24db-3def-497c-8258-dffe5d6d185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Served only\n",
    "dfp = df_scenarios.loc[df_scenarios[\"service_start\"].notna()].copy()\n",
    "\n",
    "# Post-warmup only\n",
    "warmup_cut_min = cfg.get_warmup_cut_min()\n",
    "dfp = dfp.loc[dfp[\"service_start\"] >= warmup_cut_min].copy()\n",
    "\n",
    "# Derive fields\n",
    "dfp[\"service_day\"] = (dfp[\"service_start\"] // 1440).astype(int)\n",
    "dfp[\"cat\"]         = np.where(dfp[\"kind\"].eq(\"hip\"), \"hip\", \"amb\")\n",
    "dfp[\"wait_days\"]   = dfp[\"wait\"] / (60*24)\n",
    "\n",
    "# OPTIONAL: clip display at 99th pct for readability (doesn't change stats)\n",
    "q_hi = dfp[\"wait_days\"].quantile(0.99)\n",
    "dfp[\"_wait_days_plot\"] = dfp[\"wait_days\"].clip(upper=q_hi)\n",
    "\n",
    "fig = px.violin(\n",
    "    dfp,\n",
    "    y=\"_wait_days_plot\", x=\"cat\", color=\"cat\",\n",
    "    facet_col=\"scenario\", facet_col_wrap=2,\n",
    "    box=True, points=False,\n",
    "    hover_data={\"wait_days\":\":.2f\", \"cat\":True, \"scenario\":True},\n",
    "    title=f\"Wait time of served patients (post-warmup){' — y clipped at 99th pct' if q_hi < dfp['wait_days'].max() else ''}\",\n",
    "    labels={\"_wait_days_plot\":\"Wait (days)\", \"cat\":\"Group\"}\n",
    ")\n",
    "\n",
    "# CLEAR matching (don’t pass a boolean)\n",
    "fig.for_each_yaxis(lambda ax: ax.update(matches=None))\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))  # tidy facet labels\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6523c827-b793-45b7-a53b-e6a43e793deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "served = (dfp.assign(service_day=(dfp[\"service_start\"]//1440).astype(int))\n",
    "              .groupby([\"scenario\",\"service_day\"], as_index=False)\n",
    "              .size().rename(columns={\"size\":\"served\"}))\n",
    "served[\"cum\"] = served.groupby(\"scenario\")[\"served\"].cumsum()\n",
    "\n",
    "px.line(served, x=\"service_day\", y=\"cum\", color=\"scenario\",\n",
    "        title=\"Cumulative served patients by day\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4620d9c-0456-4ca1-ad76-9a0406d8d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warmup_cut_min = cfg.get_warmup_cut_min()\n",
    "end_time_min   = cfg.get_horizon_end_min()\n",
    "\n",
    "dfb = df_scenarios.copy()\n",
    "\n",
    "# backfill breach_time where breached==True\n",
    "if \"breach_time\" not in dfb.columns:\n",
    "    dfb[\"breach_time\"] = pd.NA\n",
    "need_bt = dfb[\"breach_time\"].isna() & dfb[\"breached\"].astype(\"boolean\")\n",
    "dfb.loc[need_bt, \"breach_time\"] = (\n",
    "    dfb.loc[need_bt, \"arrival\"].astype(\"Int64\") + dfb.loc[need_bt, \"deadline\"].astype(\"Int64\")\n",
    ")\n",
    "\n",
    "bt = dfb[\"breach_time\"]\n",
    "ss = dfb[\"service_start\"]\n",
    "\n",
    "# event definition: first time crossing SLA while still waiting\n",
    "mask_event = (\n",
    "    bt.notna()\n",
    "    & ((ss.isna()) | (ss > bt))                # still waiting at breach time\n",
    "    & (bt >= warmup_cut_min) & (bt < end_time_min)  # post-warmup, inside horizon\n",
    ")\n",
    "\n",
    "events = dfb.loc[mask_event, [\"scenario\",\"rep\",\"kind\",\"breach_time\"]].copy()\n",
    "events[\"breach_day\"] = (events[\"breach_time\"] // 1440).astype(int)\n",
    "\n",
    "daily = (events.groupby([\"scenario\",\"breach_day\"], observed=False)\n",
    "               .size().rename(\"breaches\").reset_index()\n",
    "               .sort_values([\"scenario\",\"breach_day\"]))\n",
    "daily[\"cum\"] = daily.groupby(\"scenario\")[\"breaches\"].cumsum()\n",
    "\n",
    "fig = px.line(\n",
    "    daily, x=\"breach_day\", y=\"cum\", color=\"scenario\",\n",
    "    markers=True,\n",
    "    title=\"Cumulative breach events by day (post-warmup)\",\n",
    "    labels={\"breach_day\":\"Day\", \"cum\":\"Cumulative breaches\"}\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0238d81-51fa-4830-a1e7-229022db9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "events2 = events.copy()\n",
    "events2[\"group\"] = np.where(events2[\"kind\"].eq(\"hip\"), \"hip\", \"amb\")\n",
    "\n",
    "daily_g = (events2.groupby([\"scenario\",\"group\",\"breach_day\"], observed=False)\n",
    "                 .size().rename(\"breaches\").reset_index()\n",
    "                 .sort_values([\"scenario\",\"group\",\"breach_day\"]))\n",
    "daily_g[\"cum\"] = daily_g.groupby([\"scenario\",\"group\"])[\"breaches\"].cumsum()\n",
    "\n",
    "fig = px.line(\n",
    "    daily_g, x=\"breach_day\", y=\"cum\",\n",
    "    color=\"group\", facet_col=\"scenario\", facet_col_wrap=2,\n",
    "    markers=True,\n",
    "    title=\"Cumulative breach events by day — hip vs ambulatory\",\n",
    "    labels={\"breach_day\":\"Day\",\"cum\":\"Cumulative breaches\",\"group\":\"Group\"}\n",
    ")\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))  # clean facet labels\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc8b4d-ead1-4517-ac25-7c6c641636eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# events per (scenario, rep, day) from df_scenarios\n",
    "ev = df_scenarios.copy()\n",
    "ev[\"breach_day\"] = (ev[\"breach_time\"] // 1440).astype(\"Int64\")\n",
    "ev = ev[ev[\"breach_day\"].notna()].copy()\n",
    "\n",
    "# still-waiting-at-breach filter\n",
    "mask = ev[\"service_start\"].isna() | (ev[\"service_start\"] > ev[\"breach_time\"])\n",
    "ev = ev[mask]\n",
    "\n",
    "# post-warmup, within horizon\n",
    "warmup_cut = cfg.get_warmup_cut_min(); end_min = cfg.get_horizon_end_min()\n",
    "ev = ev[(ev[\"breach_time\"] >= warmup_cut) & (ev[\"breach_time\"] < end_min)]\n",
    "\n",
    "daily = (ev.groupby([\"scenario\",\"rep\",\"breach_day\"], observed=False)\n",
    "           .size().rename(\"breaches\").reset_index())\n",
    "\n",
    "# Moving average per scenario (averaged across reps)\n",
    "ma = (daily.assign(breaches_ma=lambda d: d.groupby([\"scenario\",\"rep\"])[\"breaches\"]\n",
    "                                   .transform(lambda s: s.rolling(7, min_periods=1).mean()))\n",
    "           .groupby([\"scenario\",\"breach_day\"], as_index=False)[\"breaches_ma\"].mean())\n",
    "\n",
    "import plotly.express as px\n",
    "px.line(ma, x=\"breach_day\", y=\"breaches_ma\", color=\"scenario\",\n",
    "        title=\"Daily breach incidence (7-day MA, mean across reps)\",\n",
    "        labels={\"breach_day\":\"Day\",\"breaches_ma\":\"breaches/day\"}).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf313c-e1c8-4d23-bc44-4eb6c32f2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of last vx first 60 days post warmup (averge breaches)\n",
    "last = daily[daily[\"breach_day\"] > daily[\"breach_day\"].max()-60]\n",
    "first = daily[daily[\"breach_day\"] <= daily[\"breach_day\"].min()+60]\n",
    "print(last.groupby(\"scenario\")[\"breaches\"].mean().rename(\"last60/day\"))\n",
    "print(first.groupby(\"scenario\")[\"breaches\"].mean().rename(\"first60/day\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95cbf32-c5f6-4ac9-8a88-b834ae1e0212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of last 60 days to quantify steady state\n",
    "tail = ma[ma[\"breach_day\"] >= (ma[\"breach_day\"].max()-60)]\n",
    "tail_mean = tail.groupby(\"scenario\")[\"breaches_ma\"].mean().round(2)\n",
    "print(tail_mean)  # breaches/day in steady state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c4f36b-e0e9-4991-964f-ea5435188f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4cfcd-c4b4-4380-a355-35e77b3e67a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3106f4ae-a86a-4e91-b165-b95b889537c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
